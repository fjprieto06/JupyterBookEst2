{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436f27a3-5977-4443-b81c-2cd66743b98d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <span style=\"color:brown\">The simple linear regression model</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d03bb-16a1-4763-bd4b-ee06ca9dd6c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## <span style=\"color:brown\">Contents</span> \n",
    "\n",
    "In this chapter we illustrate the procedures and properties associated with conducting a simple linear regression analysis from a sample of paired observations corresponding to two variables, which we believe may be linearly related. *Simple* refers to the fact that we will be studying the relationship between just two variables; for more than two variables we would need to use multiple linear regression models, to be introduced in a later chapter.\n",
    "\n",
    "The use of the linear regression model requires that it is clearly specified, and in particular that the parts of the model affected by the uncertainty are identified and isolated. We will consider problems such as that of finding the best representation for the model gien our data, or how to take into account the uncertainty to be able to provide meaningful measures for the variability in any use we might make of this model. We will also discuss how to measure the quality of the approximation provided by the linear regression model to the true relationship between the variables.\n",
    "\n",
    "We will cover the following topics:\n",
    "\n",
    "- Objectives of a linear regression analysis\n",
    "- Model specification\n",
    "- Least Square Estimators (LSE): construction and properties\n",
    "- Statistical inference on the linear regression model:\n",
    "  - For the slope and the intercept of the linear regression model\n",
    "  - For the variance of the errors\n",
    "  - Prediction for a new observation (actual value or average value)\n",
    "- ANOVA table for the simple linear regression model and its interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4920f9-4780-4ee9-b3e1-d8b0f5545d42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:brown\">Introduction</span>\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=color:brown;>Goals (i)</span>\n",
    "\n",
    "The preceding lessons have considered the study of parameters of one or several populations with different goals: obtaining good approximations for these values (Lesson 1), checking if these values may be close to other reference values (Lesson 2), or comparing two populations through the values of some of their parameters (Lesson 3).\n",
    "\n",
    "In this lesson we will compare two populations, as in Lesson 3, but now we will conduct this comparison from the point of view of the possible existence of a functional relationship between the values of the two random variables of interest. That is, we would like to know if it is reasonable to assume that the values of two variables might be related in a manner that is approximately linear.\n",
    "\n",
    "To do this, we will define formally the linear model that represents this linear relationship, and we will discuss how to find the best possible choice for the parameters of the relationship. As uncertainty is important for us, we will introduce distributional information to be able to associate probability information to the answers for the questions of interest, such as determining if the linear relationship is significant, or using this relationship to approximate values for one of the variables, given some information from the other variable. Finally, we will  discuss when a linear relationship identified under uncertainty may be a good model for the joint behavior of the variables of interest, and how to evaluate the quality of this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c936-9727-45fc-9b00-6287d4f7ce17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:brown\">Goals (ii)</span>\n",
    "\n",
    "Our goals for this lesson will be:\n",
    "\n",
    "- to understand the main characteristics of the available data,\n",
    "- to summarize these data in a way that is helpful to conduct the linear regression analysis,\n",
    "- to estimate the parameters of the linear regression model,\n",
    "- to obtain values useful to conduct inference on these parameters, and in particular to test the significance of the model,\n",
    "- to estimate mean responses and forecasts using the model,\n",
    "- to compute the ANOVA table associated to the model and to determine its explanatory power, and\n",
    "- to conduct some limited diagnostics on the assumptions for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccda53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## <span style=\"color:brown;\">Relationships between variables under uncertainty</span>\n",
    "\n",
    "---\n",
    "\n",
    "In Lesson 3 we have considered a case when we compare some properties of two populations, by studying the possible relationships between the values of some of their parameters, given the available data. We have used hypothesis tests to answer these questions.\n",
    "\n",
    "In many cases we wish to go beyond the relationship between parameters, and to study relationships between the individual values of two variables that we suspect are associated in some way. The joint study of their values has many advantages, as we can use the values of some of the variables to gain information about the other variables. This is particularly interesting if we do not observe these variables at the same time or with the same precision. Also, this joint study may allow us to identify relationships between the variables that may help us to understand their interaction, in order to make better decisions in the future.\n",
    "\n",
    "These relationships can take may different forms, and in general they will be represented as a mathematical function of the variables, satisfying a certain condition or conditions. For example, for two random variables $X$ and $Y$ we may be interested in finding a relationship of the form\n",
    "\n",
    "$$\n",
    "f(X,Y,U) = 0\n",
    "$$\n",
    "\n",
    "for some function $f$, which might not be known in advance, where $U$ is a random variable that represents the uncertainty in the relationship between $X$ and $Y$. In many practical cases we select in advance a functional form for $f$, but we still need to determine values for the parameters of the function; most or all of these parameters will in general be unknown and will need to be approximated from the collected observations in our samples.\n",
    "\n",
    "In addition to $X$ and $Y$, the description of $f$ includes a random variable $U$, to include the effect of the uncertainty in the relationship in practical situations. This uncertainty will be associated with the values of $X$ and $Y$, which in general will not fit exactly the model we have selected, but also with the limitations in our choice of a functional representation for this relationship, usually a simplification of the true relationship.\n",
    "\n",
    "This variable $U$ is assumed to contain no relevant information about the relationship, in the sense that the parameters of its distribution cannot contribute any information to the model. For example, we usually impose conditions such as $E[U] = 0$, and we require that $U$ should be independent of our variables $X$ and $Y$.\n",
    "\n",
    "Given observed samples for our two variables, our goal is to identify a specification of the function $f$ that offers the best possible compromise between two goals:\n",
    "\n",
    "1. Obtaining the best possible fit for our data, that is, making the values of the function $f$ in the absence of noise, $\\epsilon_i = f(X,Y)$ to be as small as possible; and\n",
    "2. using as few parameters as possible to specify the function $f$.\n",
    "\n",
    "This second condition is very important, as the errors can be made to be arbitrarily small by introducing a sufficiently complex model for the specific sample we have collected. But if we would do that, we would be selecting a model that approximates both the underlying relationship and the noise we have observed in our sample. While this underlying relationship should be useful to represent future data, in general the noise in our sample will not be related to the noise we might observe in the future. This problem is usually referred to as *overfitting*, and it happens when our model goes beyond finding a reasonable relationship between $X$ and $Y$ and also tries to explain the noise, even as we understand that this noise provides no information to model the population relationship and should not be used to define our model. Finally, note that in general we do not know how to separate the noise from the actual relationship.\n",
    "\n",
    "In practice, we usually work with models that relate the two variables in our sample (or the many variables if that is the case) so that the values of one of the variables are specified as a function of the values of the other variable. Also, we include the noise in a simple, additive or multiplicative, form to simplify its treatment. Usual models are for example\n",
    "\n",
    "$$\n",
    "Y = f(X) + U , \\quad\\text{ or }\\quad Y = U f(X)\n",
    "$$\n",
    "\n",
    "This second model might be useful when the variable $Y$ must take nonnegative values.\n",
    "\n",
    "In this lesson we will consider models that are as simple as possible. This simplest case corresponds to the model with additive noise shown above, where $f$ is being represented as a linear function. It takes the form\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X + U \n",
    "$$\n",
    "\n",
    "We call this model the *simple linear regression* model. We will devote this lesson to study how to specify the model, that is, how to find values for $\\beta_0$ and $\\beta_1$, and how to use it to generalize this relationship to other observations beyond our samples. As this model is linear, normality is a relevant theoretical property we should take into consideration: it will help us to identify relevant properties of the model and its parameters. We will analyze it assuming that our data, and in particular our errors, follow normal distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732a970c-f6ae-40c9-9e78-6570ca608fee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:brown;\">The simple linear regression model</span>\n",
    "\n",
    "---\n",
    "\n",
    "As we have mentioned, a simple linear regression model is a model that approximates the value of a (random) variable $Y$, the <span style=\"color:brown\">*dependent* or *response*</span> variable, from a linear combination of values of another variable (or variables in the case of a multiple linear regression model) $X$, the <span style=\"color:brown\">*independent* or *explanatory*</span> variable.\n",
    "\n",
    "This approximation will not provide in general an exact value for the dependent variable, as the relationship will include errors due to inaccuracies in the observed values of the variables, simplifications in the representation of the true relationships, etc. The errors in the relationship will be represented through an (unknown) random variable, $U$. These errors will be assumed to satisfy conditions that ensure they do not contain any relevant information for the model, but have some structure to simplify the theoretical analysis of this model. These conditions will be presented later as assumptions we will impose on our model.\n",
    "\n",
    "The simple linear regression model has the mathematical representation introduced above, $Y = \\beta_0 + \\beta_1 X + U$, where $\\beta_0$ (the <span style=\"color:brown\">intercept</span> of the model) and $\\beta_1$ (the <span style=\"color:brown\">slope</span>) are the parameters of the model, to be estimated from the data, and $U$ denotes the (random) errors associated with this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7689930",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### <span style=\"color:brown\">Observations and the simple linear regression model</span>\n",
    "\n",
    "The treatment of the model starts with the information contained in a collection of values for the variables corresponding to a simple random sample of paired values $\\{ (x_i , y_i )\\}_{i=1}^n$. Based on this information, we usually proceed through the following steps:\n",
    "\n",
    "1. Identify appropriate estimators for the parameters $\\beta_j$, $j = 0,1$, and use these estimators to obtain estimates based on the available the data (<span style=\"color:brown\">*estimation*</span>).\n",
    "2. Interpret the values of these estimates with respect to their population, based on their distributions, to obtain confidence intervals or to conduct significance tests, for example (<span style=\"color:brown\">*inference*</span>).\n",
    "3. Use the model and the estimated parameters to obtain information about some population values of interest, for example, mean values of the response variable outside of the sample (<span style=\"color:brown\">*forecasting*</span>).\n",
    "\n",
    "In our treatment of this model we will assume that the values of the variable $X$ are known, that is, they will not be considered as observations from a random variable, but rather as known values. This represents situations where we observe $X$ before we observe $Y$, or when we observe $X$ with very little variability, compared to our observations for $Y$. Formally, we will conduct all the relevant analysis and evaluations related to this model, conditional on the values of the variable $X$ given in our sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a026554-8164-4127-b95c-fd31d34a99f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:blue;\">Preparing R and the data</span>\n",
    "\n",
    "---\n",
    "\n",
    "To illustrate the concepts we have introduced, and to motivate possible choices of good estimators, we will consider specific examples, mostly based on real data, which we will process using <span style=\"color:blue;font-family:monospace;font-size:90%;\">R</span>.\n",
    "\n",
    "We start by preparing <span style=\"color:blue;font-family:monospace;font-size:90%;\">R</span> to read and manipulate the data mentioned above. In the following <span style=\"color:blue;font-family:monospace;font-size:90%;\">R</span> <span style=\"color:brown\">code cell</span> we:\n",
    "\n",
    "1. Load the <span style=\"color:blue;font-family:monospace;font-size:90%;\">R</span> libraries we are going to need for our examples.\n",
    "2. Define a function, <span style=\"color:blue;font-family:monospace;font-size:90%;\">table_prnt</span>, specifying the format for the tables that will present the numerical results in this lesson.\n",
    "3. Introduce information to work with the available data sets.\n",
    "\n",
    "The <span style=\"color:brown;\">available data sets</span> and their identifying codes are:\n",
    "\n",
    "1. Hourly prices for the Iberian electricity market\n",
    "2. Grades for a Statistics subject in UC3M\n",
    "3. Share prices for a company (Iberdrola) from the IBEX index\n",
    "4. Simulated data from a N(80,30) distribution (var 1), an Exp(lambda=1/30) distribution (var 2) and a Binom(20,0.4) distribution (var 3)\n",
    "5. Data from the Sustainable Develpment Report 2021, with the scores by country for goals 1 and 2\n",
    "\n",
    "In order to add another data set to this collection, you should include information for each of the following variables: the <span style=\"color:blue;font-family:monospace;font-size:90%;\">.csv</span> file containing the data and a text with a short description for the data.\n",
    "\n",
    "It is also important to ensure that the <span style=\"color:brown;\">working directory</span> has been <span style=\"color:brown;\">selected correctly,</span> as the directory that includes all the data sets that could be used in this lesson.\n",
    "\n",
    "To execute the commands in the cell, select the cell by clicking on it, and then <span style=\"color:blue;\">press the **RUN** button</span> in the menu bar, or press <span style=\"color:blue;\">Shift-Enter.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d114074-79dd-47a8-9daf-12e5244cd6cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#options(jupyter.plot_mimetypes = c(\"text/plain\",\"image/png\"))\n",
    "\n",
    "# Load libraries with R functions\n",
    "\n",
    "suppressMessages(library(tidyverse))\n",
    "suppressMessages(library(huxtable))\n",
    "library(knitr)\n",
    "suppressMessages(library(kableExtra))\n",
    "library(IRdisplay)\n",
    "suppressMessages(library(gridExtra))\n",
    "suppressMessages(library(qqplotr))\n",
    "suppressMessages(library(GGally))\n",
    "suppressMessages(library(car))\n",
    "library(grid)\n",
    "\n",
    "# Define a function to format and print the results of interest\n",
    "\n",
    "outp.type = 0   # = 1 for html output, = 0 for Jupyter Books\n",
    "\n",
    "if (outp.type == 1) {\n",
    "    table_prnt <- function(p.df,p.capt) {\n",
    "    # A function to control the presentation of tables with numerical summaries\n",
    "    p.df %>% kable(\"html\",caption=paste0('<em>',p.capt,'</em>'),align='r') %>%\n",
    "    kable_styling(full_width = F, position = \"left\") %>% as.character() %>% display_html()\n",
    "    }\n",
    "    } else {\n",
    "    table_prnt <- function(p.df,p.capt) {\n",
    "    # A function to control the presentation of tables with numerical summaries\n",
    "    p.df %>% kable(\"simple\",caption=p.capt,align='r')\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6288e-f0de-458e-ae27-e7434a5e8b47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:blue;\">Examples based on external data sources</span>\n",
    "\n",
    "---\n",
    "\n",
    "To illustrate the concepts we have introduced, and to motivate possible choices of good estimators, we will consider specific examples, mostly based on real data, which we will process using <span style=\"color:blue;font-family:monospace;font-size:90%;\">R</span>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684eab9f-bcce-406a-b2b1-f6e650e98686",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "#### <span style=\"color:blue;\">Selecting and displaying the data set and the variables of interest</span>\n",
    "\n",
    "We select one of these data sets and two variables of interest in the following cell.\n",
    "\n",
    "1. We assign the corresponding number to the variable <span style=\"color:blue;font-family:monospace;font-size:90%;\">sel.data</span>, at the start of the following code cell.\n",
    "2. We read the file and include the data in a <span style=\"color:brown;\">data frame</span> with the name <span style=\"color:blue;font-family:monospace;font-size:90%;\">Data.fr</span>.\n",
    "3. We assign the numbers corresponding to the order of the two variables of interest for our linear model in the data set, to the variable <span style=\"color:blue;font-family:monospace;font-size:90%;\">sel.col</span>. The last value of <span style=\"color:blue;font-family:monospace;font-size:90%;\">sel.col</span> will be assumed to correspond to the <span style=\"color:brown;\">dependent variable.</span>\n",
    "4. We then assign the values of the variables to two <span style=\"color:blue;font-family:monospace;font-size:90%;\">R</span> variables with the names <span style=\"color:blue;font-family:monospace;font-size:90%;\">data.sel.x</span>, our <span style=\"color:brown;\">independent variable,</span> and <span style=\"color:blue;font-family:monospace;font-size:90%;\">data.sel.y</span>, our <span style=\"color:brown;\">dependent variable.</span> The values of both these variables are assigned to a single data frame, <span style=\"color:blue;font-family:monospace;font-size:90%;\">data.sel</span>.\n",
    "\n",
    "Finally, we print the names of the selected data set and variables, to check that these values are the correct ones. Then we display a part of the values from the <span style=\"color:blue;font-family:monospace;font-size:90%;\">.csv</span> file, keeping the same structure of the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02708353-f52b-43da-9792-9e3128ed96fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the data set of interest\n",
    "\n",
    "## Datasets that are available for this lesson\n",
    "\n",
    "v.pref = data.frame(file = c(\"Dat_PreciosOMIE.csv\",     # Name of the .csv data file\n",
    "                            \"Dat_Calificaciones.csv\",\n",
    "                            \"Dat_PreciosIBE_MC.csv\",\n",
    "                            \"Dat_SimulatedData.csv\",\n",
    "                            \"Dat_SDR21.csv\"))\n",
    "v.pref$title = c(\"Electricity prices\",         # Short title for the data\n",
    "                \"Grades\",\n",
    "                \"Share returns\",\n",
    "                \"Simulated data\",\n",
    "                \"SDG 2021 Scores\")\n",
    "\n",
    "## Indicate the data set and variable to select\n",
    "## These values can be modified\n",
    "\n",
    "sel.data = 2\n",
    "sel.col = c(1,3)\n",
    "\n",
    "## Read the data\n",
    "\n",
    "s.pref = v.pref[sel.data,]\n",
    "Data.fr = read.csv2(s.pref$file)\n",
    "\n",
    "n.dat = nrow(Data.fr)\n",
    "data.sel.x = Data.fr[,sel.col[1]]\n",
    "data.sel.y = Data.fr[,sel.col[2]]\n",
    "data.sel = data.frame(X=data.sel.x, Y=data.sel.y)\n",
    "c.names = colnames(Data.fr)[c(sel.col[1],sel.col[2])]\n",
    "vr.1 = c(rep(\"VarX\",n.dat),rep(\"VarY\",n.dat))\n",
    "vr.2 = c(data.sel.x,data.sel.y)\n",
    "val.melt = data.frame(variable = vr.1, value = vr.2)\n",
    "\n",
    "## Summary of the selected data\n",
    "\n",
    "descr.df = as.data.frame(c(s.pref$title,c.names))\n",
    "colnames(descr.df) <- c(\"Selection\")\n",
    "rownames(descr.df) <- c(\"Data set\",\"Variable X\",\"Variable Y\")\n",
    "\n",
    "Data.hux.0 <-\n",
    "  hux(descr.df) %>%\n",
    "  set_bold(row = 1, value = T) %>%\n",
    "  set_all_borders(TRUE)\n",
    "table_prnt(Data.hux.0[-1,],\"Selected data and variables\")\n",
    "\n",
    "# Print a part of the data we have selected\n",
    "\n",
    "max.row.show = 8       # Max number of individual values to show\n",
    "max.col.show = 8       # Max number of variables to show\n",
    "\n",
    "n.row.show = min(nrow(Data.fr),max.row.show)\n",
    "n.col.show = min(ncol(Data.fr),max.col.show)\n",
    "\n",
    "Data.hux.1 <-\n",
    "  hux(Data.fr[1:n.row.show,1:n.col.show]) %>%\n",
    "  set_bold(row = 1, value = T) %>%\n",
    "  set_all_borders(TRUE)\n",
    "rownames(Data.hux.1) <- c(0:n.row.show)\n",
    "table_prnt(Data.hux.1[-1,],s.pref$title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb7b97-cd18-405c-b83f-5aa2e67d3561",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "#### <span style=\"color:blue;\">Summaries for the sample data</span>\n",
    "\n",
    "In the following cell we conduct some simple exploratory analysis of the data from the variables we have selected. We start by computing some of their most relevant numerical summaries, such as their means, standard deviations and medians.\n",
    "\n",
    "We also draw a boxplot of the sample data corresponding to these two variables, as well as a scatterplot of the values of the variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b39b0-24b9-4e5d-9d8e-ed9d504df709",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print summaries from the selected data set\n",
    "\n",
    "smp.mn.x = mean(data.sel.x)\n",
    "smp.mn.y = mean(data.sel.y)\n",
    "smp.sd.x = sd(data.sel.x)\n",
    "smp.sd.y = sd(data.sel.y)\n",
    "smp.med.x = median(data.sel.x)\n",
    "smp.med.y = median(data.sel.y)\n",
    "Sum.fr = as.data.frame(round(matrix(c(n.dat,smp.mn.x,smp.med.x,smp.sd.x,\n",
    "                                      n.dat,smp.mn.y,smp.med.y,smp.sd.y),4,2),3))\n",
    "\n",
    "Data.hux.2 <-\n",
    "  hux(Sum.fr) %>%\n",
    "  set_bold(row = 1, value = T) %>%\n",
    "  set_all_borders(TRUE)\n",
    "\n",
    "rownames(Data.hux.2) <- c(\"\",\"Sample size\",\"Mean\",\"Median\",\"Standard deviation\")\n",
    "colnames(Data.hux.2) <- c(\"Values X\",\"Values Y\")\n",
    "table_prnt(Data.hux.2[-1,],sprintf(\"%s summary\",s.pref$title))\n",
    "\n",
    "## Boxplot for the data\n",
    "\n",
    "plt.bxpl = val.melt %>% ggplot(aes(x=variable,y=value)) + geom_boxplot() +\n",
    "  ggtitle(sprintf(\"Boxplot %s\",s.pref$title)) +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5)) +\n",
    "  scale_x_discrete(labels=c(\"VarX\" = c.names[1], \"VarY\" = c.names[2]))\n",
    "plot(plt.bxpl) \n",
    "\n",
    "## Scatterplot for the data\n",
    "\n",
    "plt.scat = data.sel %>% ggplot(aes(x=X,y=Y)) + geom_point() +\n",
    "  ggtitle(sprintf(\"Scatterplot %s\",s.pref$title)) +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "plot(plt.scat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8d39f-3850-478d-8bd0-ebc5a9d70945",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## <span style=\"color:brown;\">Parameter estimation: the least squares method</span>\n",
    "\n",
    "---\n",
    "\n",
    "In the procedure we have described for this lesson, the first step to conduct the analysis of a linear regression model is the identification of estimators for the parameters in the model, $\\beta_0$ and $\\beta_1$. We will obtain these estimators by applying a procedure known as the <span style=\"color:brown\">least squares method</span> to the available sample data.\n",
    "\n",
    "This procedure is not the only one that could be applied to find these estimators, but it is the most common one and it has very good theoretical properties. See [Appendix 1](#App4_1) for other possible estimation procedures.\n",
    "\n",
    "This method aims to find values, the estimates $\\hat \\beta_0$ and $\\hat \\beta_1$, which will yield the smallest possible errors in the model (using these estimates) for our sample values. Formally, we define the error estimate associated to an observation as $e_i \\equiv y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i$; we refer to this error as the <span style=\"color:brown\">residual</span> for observation $i$. Note that as we take the values of the independent variable as given, we are looking at the errors associated to the *values of the response variable*, that is, the errors in the values provided by the model for each observation $i$, $\\hat \\beta_0 - \\hat \\beta_1 x_i$, when compared with the corresponding observed values of $Y$, $y_i$.\n",
    "\n",
    "Formally, we define the problem whose solutions are the parameter values yielding the smallest errors as\n",
    "\n",
    "$$\n",
    "\\min_{\\hat \\beta_0,\\hat \\beta_1} \\sum_{i=1}^n e_i (\\hat \\beta_0,\\hat \\beta_1)^2 = \\min_{\\hat \\beta_0,\\hat \\beta_1} \\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "If we use the first-order optimality conditions for this problem to obtain the optimal values for $\\hat \\beta_0$ and $\\hat \\beta_1$, we have that its unique optimal solution (the least squares estimators for the parameters) is given by\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{rcl}\n",
    "\\hat \\beta_1 & = & \\displaystyle \\frac{\\text{cov}(x,y)}{s_x^2} \\\\\n",
    "\\hat \\beta_0 & = & \\bar y - \\hat \\beta_1 \\bar x\n",
    "\\end{array} \\right\\}\n",
    "$$\n",
    "\n",
    "These formulas define the least squares estimators for the parameters of the simple linear regression model. See [Appendix 2](#App4_2) for their formal derivation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27681c32-5923-473e-8725-0c2a1427b072",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### <span style=\"color:green;\">Questions</span>\n",
    "\n",
    "<span style=\"color:green\">Answer the following questions:</span>\n",
    "- <span style=\"color:green\">For these least-squares estimates, should the resulting line leave the same number of observations above and below it?</span>\n",
    "- <span style=\"color:green\">Assume that there is no relationship between the variables, how how would the least-squares line look like? Would it be a horizontal line? Why?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768b34b9-a7ea-4290-8032-f9dd0e6c2ebc",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style=\"color:blue\">Least squares: a numerical example</span>\n",
    "\n",
    "The following cell shows the values of the estimates obtained using the preceding formulas, as well as the plot of the regression line for the data we have selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b19ab8c-b317-4181-8853-6e01efd4dc65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the regression line computed using least squares\n",
    "\n",
    "## Parameter values\n",
    "\n",
    "hat.beta.1 = cov(data.sel.x,data.sel.y)/var(data.sel.x)\n",
    "hat.beta.0 = mean(data.sel.y) - hat.beta.1*mean(data.sel.x)\n",
    "lr.par.df = as.data.frame(round(matrix(c(hat.beta.1,hat.beta.0),2,1),3))\n",
    "\n",
    "Data.hux.3 <-\n",
    "  hux(lr.par.df) %>%\n",
    "  set_bold(row = 1, value = T) %>%\n",
    "  set_all_borders(TRUE)\n",
    "\n",
    "rownames(Data.hux.3) <- c(\"\",\"Slope\",\"Intercept\")\n",
    "colnames(Data.hux.3) <- c(\"Values\")\n",
    "table_prnt(Data.hux.3[-1,],\"Regression params\")\n",
    "\n",
    "## Scatterplot for the data\n",
    "\n",
    "data.sel.yh = hat.beta.0 + hat.beta.1*data.sel.x\n",
    "data.sel$Yest = data.sel.yh\n",
    "\n",
    "plt.scat.2 = data.sel %>% ggplot() + geom_point(aes(x=X,y=Y)) +\n",
    "  geom_line(aes(x=X,y=Yest),color=\"blue\",linewidth=0.75) +\n",
    "  ggtitle(sprintf(\"Linear regr %s vs %s\",c.names[2],c.names[1])) +\n",
    "  labs(y = c.names[2], x = c.names[1]) +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "\n",
    "plt.scat.3 = data.sel %>% ggplot() + geom_point(aes(x=X,y=Y)) +\n",
    "  geom_line(aes(x=X,y=Yest),color=\"blue\",linewidth=0.75) +\n",
    "  geom_segment(aes(x=X,y=Yest,xend=X,yend=Y),color=\"red\",linewidth=0.5) +\n",
    "  ggtitle(sprintf(\"%s vs %s residuals\",c.names[2],c.names[1])) +\n",
    "  labs(y = c.names[2], x = c.names[1]) +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "\n",
    "suppressWarnings(grid.arrange(plt.scat.2,plt.scat.3,nrow = 1,\n",
    "                              top=textGrob(sprintf(\"Linear regression fit %s\",s.pref$title),\n",
    "                                           gp=gpar(fontsize=15,col=\"blue\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100bd202-b7ab-4b15-b318-8aa00acfab28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### <span style=\"color:brown;\">A procedure to compute the least squares estimates</span>\n",
    "\n",
    "#### <span style=\"color:brown;\">Basic sample information and relevant statistics</span>\n",
    "\n",
    "In this section we describe a procedure to compute the values of the parameter estimates for the simple linear regression model, which we will be able to apply to the manual, step-by-step solution of practical exercises. Later on we will see how to conduct these computations using the tools available in R, similar to those present in many other data analysis software packages.\n",
    "\n",
    "We note that, in order to obtain the values of these estimates, we do not need all the information in the sample; it is enough that we collect and use some summaries of this data. Some basic information that would allow us to conduct the computations in the formulas we have introduced would include the values of:\n",
    "\n",
    "- the number of observations, $n$\n",
    "- the sums of the values of the two variables, $\\sum_{i=1}^n x_i$ and $\\sum_{i=1}^n y_i$\n",
    "- the sums of the squares of these values, $\\sum_{i=1}^n x_i^2$ and $\\sum_{i=1}^n y_i^2$\n",
    "- the sum of the crossproducts of the two variables, $\\sum_{i=1}^n x_i y_i$\n",
    "\n",
    "From these values, we can compute the values of the basic sample statistics: means, quasivariances and covariance, which we will use to estimate the values of the linear regression parameters. The formulas for these statistics are:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rclrcl}\n",
    "\\bar x & = & \\displaystyle \\frac{1}{n} \\sum_{i=1}^n x_i, & \\quad \\displaystyle \\bar y & = & \\displaystyle \\frac{1}{n} \\sum_{i=1}^n y_i ,\\\\\n",
    "s_x^2 & = & \\displaystyle \\frac{1}{n-1} \\left( \\sum_{i=1}^n x_i^2 - n\\bar x^2 \\right) , & \\quad\n",
    "s_y^2 & = & \\displaystyle \\frac{1}{n-1} \\left( \\sum_{i=1}^n y_i^2 - n\\bar y^2 \\right) , \\\\\n",
    "\\mbox{cov}(x,y) & = & \\displaystyle \\frac{1}{n-1} \\left( \\sum_{i=1}^n x_i y_i - n\\bar x\\bar y \\right) &\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For the selected data we obtain the values shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d6ab2-7024-40bd-ab63-d87718fed6b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Summary of the data in the data frame\n",
    "\n",
    "n.obs = nrow(data.sel)\n",
    "sum.x = sum(data.sel$X)\n",
    "sum.y = sum(data.sel$Y)\n",
    "sum.x2 = sum(data.sel$X^2)\n",
    "sum.y2 = sum(data.sel$Y^2)\n",
    "sum.xy = sum(data.sel$X*data.sel$Y)\n",
    "\n",
    "rn.1 <- 'Number of obs'\n",
    "rn.2 <- sprintf('Sum of %s',c.names[1])\n",
    "rn.3 <- sprintf('Sum of %s',c.names[2])\n",
    "rn.4 <- sprintf('Sum of sq %s',c.names[1])\n",
    "rn.5 <- sprintf('Sum of sq %s',c.names[2])\n",
    "rn.6 <- sprintf('Sum of %s * %s',c.names[1],c.names[2])\n",
    "\n",
    "val.0 <- c(n.obs,sum.x,sum.y,sum.x2,sum.y2,sum.xy)\n",
    "out.0 <- as.data.frame(matrix(val.0,length(val.0),1))\n",
    "colnames(out.0) = c(\"Values\")\n",
    "rownames(out.0) = c(rn.1, rn.2, rn.3, rn.4, rn.5, rn.6)\n",
    "\n",
    "table_prnt(out.0,\"Summaries of our sample data\")\n",
    "\n",
    "# Values of means, variances and covariance\n",
    "\n",
    "mn.x = sum.x/n.obs\n",
    "mn.y = sum.y/n.obs\n",
    "s2.x = (sum.x2 - n.obs*mn.x^2)/(n.obs-1)\n",
    "s2.y = (sum.y2 - n.obs*mn.y^2)/(n.obs-1)\n",
    "cov.xy = (sum.xy - n.obs*mn.x*mn.y)/(n.obs-1)\n",
    "\n",
    "rn.2 <- sprintf('Mean of %s',c.names[1])\n",
    "rn.3 <- sprintf('Mean of %s',c.names[2])\n",
    "rn.4 <- sprintf('Quasivar of %s',c.names[1])\n",
    "rn.5 <- sprintf('Quasivar of %s',c.names[2])\n",
    "rn.6 <- sprintf('Covar of %s %s',c.names[1],c.names[2])\n",
    "\n",
    "val.0 <- round(c(mn.x,mn.y,s2.x,s2.y,cov.xy),3)\n",
    "out.0 <- as.data.frame(matrix(val.0,length(val.0),1))\n",
    "colnames(out.0) = c(\"Values\")\n",
    "rownames(out.0) = c(rn.2, rn.3, rn.4, rn.5, rn.6)\n",
    "\n",
    "table_prnt(out.0,\"Basic statistics for our data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde7f5e-6db4-42d8-86e4-7d8526602a11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "#### <span style=\"color:brown;\">Regression parameter estimates</span>\n",
    "\n",
    "Based on the preceding information, we are now able to compute the values for the linear regression coefficients, given our data, using the *least squares method*. The formulas we should apply are\n",
    "\n",
    "$$\n",
    "\\hat \\beta_1 = \\frac{\\mbox{cov}(x,y)}{s_x^2} , \\qquad \\hat \\beta_0 = \\bar y - \\hat \\beta_1 \\bar x .\n",
    "$$\n",
    "\n",
    "In addition to the optimality properties derived from the procedure used to define the estimators, and as $\\bar y = \\hat \\beta_0 + \\hat \\beta_1 \\bar x$, it also holds that:\n",
    "\n",
    "- The point $(\\bar x,\\bar y)$ always lies on the linear regression line.\n",
    "- The sum of the residuals always satisfies\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n e_i = \\sum_{i=1}^n \\left( y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i \\right) = n \\bar y - n \\hat \\beta_0 - \\hat \\beta_1 n \\bar x = 0\n",
    "$$\n",
    "\n",
    "For our sample observations we obtain the estimates shown in the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56453fe4-e336-46d6-9115-87e454c24aa0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters of the model\n",
    "\n",
    "hat.beta.1 = cov(data.sel.x,data.sel.y)/var(data.sel.x)\n",
    "hat.beta.0 = mean(data.sel.y) - hat.beta.1*mean(data.sel.x)\n",
    "lr.par.df = as.data.frame(round(matrix(c(hat.beta.1,hat.beta.0),2,1),3))\n",
    "\n",
    "Data.hux.3 <-\n",
    "  hux(lr.par.df) %>%\n",
    "  set_bold(row = 1, value = T) %>%\n",
    "  set_all_borders(TRUE)\n",
    "\n",
    "rownames(Data.hux.3) <- c(\"\",\"Slope\",\"Intercept\")\n",
    "colnames(Data.hux.3) <- c(\"Estimates\")\n",
    "table_prnt(Data.hux.3[-1,],\"Regression parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f9501-46ac-450c-94d8-7faf1880c167",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style=\"color:red\">Exercise</span>\n",
    "\n",
    "*A fast-food company wants to evaluate the relationship between the number of ads in a social media app it purchases each week ($x$) and the profits in euros from its home-delivery sales during the week ($y$). From the observed data corresponding to a sample of 19 observations (weeks) we have the following information:*\n",
    "\n",
    "$$\n",
    "   \\bar x = 636.5 , \\quad \\bar y = 1757.502 , \\quad \\sum_{i=1}^{19} x_i^2 = 8073424 , \\quad \\sum_{i=1}^{19} x_i y_i = 21629441\n",
    "$$\n",
    "\n",
    "*Estimate the regression line for the weekly profits in terms of the number of social media ads.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ed99a-fdb0-4fca-8ca0-3aa02128dd79",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "##### <span style=\"color:red\">Exercise. Solution</span>\n",
    "\n",
    "We define our variables $X =$ \"Number of ads distributed in a week\" and $Y =$ \"Weekly profit in euros\"\n",
    "\n",
    "To estimate the simple linear regression model of interest,\n",
    "\n",
    "$$\n",
    "    y_i = \\beta_0 + \\beta_1 x_i + u_i ,\n",
    "$$\n",
    "\n",
    "we will make use of the least squares estimators formulas. But before we do that, we will compute the values that will be required to replace in the formulas:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "s_x^2 & = & \\displaystyle \\frac{\\sum_i x_i^2 - n\\bar x^2}{n-1} = \\frac{8073424 - 19\\times 636.5^2}{18} = 20883.96 \\\\\n",
    "\\text{cov}(x,y) & = & \\displaystyle \\frac{\\sum_i x_i y_i - n\\bar x\\bar y}{n-1} = \\frac{21629441 - 19\\times 636.5\\times 1757.502}{18} = 20838.36\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The estimates for the parameters are given by\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\hat \\beta_1 & = & \\displaystyle \\frac{\\text{cov}(x,y)}{s_x^2} = \\frac{20838.36}{20883.96} = 0.9978 , \\\\\n",
    "\\hat \\beta_0 & = & \\bar y - \\hat \\beta_1 \\bar x = 1757.502 - 0.9978\\times 636.5 = 1122.391\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Thus, the estimated regression line is given by:\n",
    "\n",
    "$$\n",
    "   \\hat y_i = 1122.391 + 0.9978 x_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976baf83-92d5-4106-a730-6ed3ffb0ba1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## <span style=\"color:brown;\">Inference for the simple linear regression model</span>\n",
    "\n",
    "---\n",
    "\n",
    "Once we have estimated the parameters of our linear regression model from the observations in our sample, we would like to conduct inference on these parameters, and in general on any results we may obtain from this model. For example, as our estimates are based on a random sample, they will take different (random) values for different samples. We would like to provide some information on the variability of these possible random values, on their distribution and on some properties of interest of their (population) parameters.\n",
    "\n",
    "We will start with an exploratory analysis, based on the estimates we would obtain from random subsamples generated from our selected data. We may notice regularities in the distribution of these values, for example by looking at their histograms. The following plot shows these histograms, with the main aim of characterizing the distributions associated to the parameter estimators we have defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475deb4-4308-41f5-a8ea-6ad8e83c254b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define regression estimates based on random samples\n",
    "\n",
    "smp.sz.prop = 0.4\n",
    "smp.sz = floor(smp.sz.prop*n.dat)\n",
    "\n",
    "n.reps = 100\n",
    "n.bins = 18\n",
    "\n",
    "## Extract subsamples from the data and compute estimates\n",
    "\n",
    "param.sel = NULL\n",
    "for (ix in 1:n.reps) {\n",
    "    ix.sel = sample(n.dat,smp.sz)\n",
    "    data.sel.ss = data.sel[ix.sel,]\n",
    "    beta.1.ss = cov(data.sel.ss$X,data.sel.ss$Y)/var(data.sel.ss$X)\n",
    "    beta.0.ss = mean(data.sel.ss$Y) - beta.1.ss*mean(data.sel.ss$X)\n",
    "    param.sel = rbind(param.sel,c(beta.0.ss,beta.1.ss))\n",
    "}\n",
    "param.sel = data.frame(param.sel)\n",
    "colnames(param.sel) = c(\"beta0\",\"beta1\")\n",
    "\n",
    "## Histograms for the collected data\n",
    "\n",
    "plt.hist.0 = param.sel %>% ggplot(aes(x=beta0)) + geom_histogram(bins=n.bins,alpha=0.7) +\n",
    "  ggtitle(\"Histogram beta0\") +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "plt.hist.1 = param.sel %>% ggplot(aes(x=beta1)) + geom_histogram(bins=n.bins,alpha=0.7) +\n",
    "  ggtitle(\"Histogram beta1\") +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "\n",
    "suppressWarnings(grid.arrange(plt.hist.1,plt.hist.0,nrow = 1,\n",
    "                              top=textGrob(sprintf(\"%s Linear regression parameters\",s.pref$title),gp=gpar(fontsize=15,col=\"blue\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc67595-a2e4-49c4-94a8-018d2126358e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### <span style=\"color:brown;\">Assumptions on the simple linear regression model</span> \n",
    "\n",
    "To be able to conduct these inference procedures, we need to make use of distributional information related to our data. We start by introducing some assumptions on our population, and in particular on the distribution of the uncertainty in the errors $U$. \n",
    "\n",
    "Given a random sample $\\{(X_i,Y_i)\\}$, let $Y_i = \\beta_0 + \\beta_1 X_i + U_i$. Our basic assumptions for the linear regression model will be the following:\n",
    "\n",
    "1. There exists a linear relationship between the variables $X$ and $Y$, as opposed to these variables being related through some nonlinear relationship.\n",
    "2. The errors in the model follow a normal distribution $U \\sim N(0,\\sigma^2)$ where $\\sigma^2$, the variance of the errors, is some unknown scalar value that does not depend on the sample values.\n",
    "3. The errors in the model corresponding to different values of $X$ are independent, that is, $U | (X = x)$ is independent of $U | (X = x')$ for $x \\not= x'$. In particular, $U_i = U | (X = x_i)$ is independent of $U_j = U | (X = x_j)$ for any $i$ and any $j \\not= i$.\n",
    "\n",
    "These assumptions, and in particular the ones related to the distribution of the errors, allow us to obtain information on the distribution of the different parameters of the model, the forecasts we may generate from this model, etc. And in general, to conduct statistical inference on these parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3cd30-5342-4c7a-8bfa-c6ce3e1a8856",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "#### <span style=\"color:blue\">Checking the assumptions for our data</span>\n",
    "\n",
    "In the following cell we check how well (some of) the preceding assumptions would apply to our (real) data. We present a histogram and a Q-Q plot for the residuals in our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44b6c8-9bea-40e4-b687-d412dfafb4b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "\n",
    "## This value can be modified\n",
    "\n",
    "n.bins = 15\n",
    "\n",
    "## Values of the residuals and graphical representations\n",
    "\n",
    "data.sel$Res = data.sel$Y - data.sel$Yest\n",
    "\n",
    "plt.res.s = data.sel %>% ggplot(aes(x=Yest,y=Res)) + geom_point() +\n",
    "  ggtitle(sprintf(\"Scatterplot estimates vs residuals %s\",s.pref$title)) +\n",
    "  xlab(\"Estimates\") + ylab(\"Residuals\") +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "\n",
    "plt.res.h = data.sel %>% ggplot(aes(x=Res)) + geom_histogram(bins=n.bins,alpha=0.7) +\n",
    "  ggtitle(sprintf(\"Histogram residuals %s\",s.pref$title)) +\n",
    "  xlab(\"Residuals\") + ylab(\"Frequencies\") +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "\n",
    "plt.res.q <- data.sel %>% ggplot(aes(sample=Res)) +\n",
    "  stat_qq_band() + stat_qq_line(color=\"red\") + stat_qq_point() +\n",
    "  ggtitle(sprintf(\"QQplot residuals %s\",s.pref$title)) +\n",
    "  ylab(\"Residual quantiles\") + xlab(\"Normal distribution quantiles\") +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "\n",
    "suppressWarnings(grid.arrange(plt.res.s,plt.res.h,plt.res.q,ncol = 2,\n",
    "                              top=textGrob(sprintf(\"%s residuals normality check\",s.pref$title),gp=gpar(fontsize=15,col=\"blue\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee354cc-e46b-49c7-8aaf-1d26ac4f3a31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "#### <span style=\"color:brown;\">Regression parameter estimates (ii)</span>\n",
    "\n",
    "As part of our regression model assumptions, we have introduced a third parameter for our model, the <span style=\"color:brown\">variance of the errors,</span> $\\sigma^2 = \\text{Var}(U)$. This parameter is unknown and will need to be estimated from the sample data.\n",
    "\n",
    "We define our estimator for this third parameter from the (sample) variance of the residuals $e_i$. These residuals represent our sample estimates for the errors in the model $U$. As the mean of the residuals is always equal to zero, $\\bar e = 0$, this variance will be given by\n",
    "\n",
    "$$\n",
    "\\hat \\sigma_R^2 = \\frac{1}{n} \\sum_{i=1}^n (e_i - \\bar e)^2 = \\frac{1}{n} \\sum_{i=1}^n e_i^2\n",
    "$$\n",
    "\n",
    "As in the case of the sample variance, this estimator of the variance of the errors is not unbiased. To obtain  an unbiased estimator we have to divide the sum of the squares by $n-2$, instead of $n$. An intuitive explanation is that in order to compute $e_i$ we have had to estimate two parameters, $\\hat \\beta_0$ and $\\hat \\beta_1$, from the sample data, and as a consequence we lose two degrees of freedom.\n",
    "\n",
    "Our <span style=\"color:brown\">unbiased estimator</span> for the variance of the errors, which we will refer to as the <span style=\"color:brown\">*residual variance*,</span> $s_R^2$, is defined as\n",
    "\n",
    "$$\n",
    "s_R^2 = \\frac{1}{n-2} \\sum_{i=1}^n e_i^2 .\n",
    "$$\n",
    "\n",
    "A justification for the unbiasedness of $s_R^2$ can be found in [Appendix 3](#App4_3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435bb2b9-7827-4292-9188-1690bcff25a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters of the model: variance of the errors\n",
    "\n",
    "## Sum of the squared residuals\n",
    "\n",
    "data.sel$Res = data.sel$Y - data.sel$Yest\n",
    "sum.e2 = sum(data.sel$Res^2)\n",
    "\n",
    "n.obs = nrow(data.sel)\n",
    "df.m = 1                 # Number of independent variables\n",
    "df.r = n.obs - 1 - df.m\n",
    "sR.2 = sum.e2/df.r\n",
    "\n",
    "rn.3 <- \"Sum of sq residuals\"\n",
    "rn.4 <- \"Residual variance\"\n",
    "val.0 <- c(sum.e2,sR.2)\n",
    "out.0 <- round(as.data.frame(matrix(val.0,length(val.0),1)),3)\n",
    "colnames(out.0) = c(\"Values\")\n",
    "rownames(out.0) = c(rn.3,rn.4)\n",
    "\n",
    "table_prnt(out.0,\"Parameter estimates\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e245753-d0b6-4e26-a3f3-a75d1d7ce9d6",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style=\"color:red\">Exercise</span>\n",
    "\n",
    "*A fast-food company wants to evaluate the relationship between the number of ads in a social media app it purchases each week ($x$) and the profits in euros from its home-delivery sales during the week ($y$). From the observed data corresponding to a sample of 19 observations (weeks) we have the following information:*\n",
    "\n",
    "$$\n",
    "\\bar x = 636.5 , \\quad \\bar y = 1757.502 , \\quad \\sum_{i=1}^{19} x_i^2 = 8073424 , \\quad \\sum_{i=1}^{19} x_i y_i = 21629441\n",
    "$$\n",
    "\n",
    "- *Estimate the regression line for the weekly profits in terms of the number of social media ads.*\n",
    "\n",
    "- *Compute an estimate for the variance of the errors in the regression model, knowing that the sum of squares of the residuals for this model is*\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{19} e_i^2 = 29821\n",
    "$$\n",
    "\n",
    "- *Conduct again this last computation assuming now that you are not given the value of $\\sum_i e_i^2$, but you know that*\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{19} y_i^2 = 59091545\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dccf68-5bf4-4f63-a851-157844d8b70c",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "##### <span style=\"color:red\">Exercise. Solution</span>\n",
    "\n",
    "We define our variables $X =$ \"Number of ads distributed in a week\" and $Y =$ \"Weekly profit in euros\"\n",
    "\n",
    "We have already obtained the estimated regression line as:\n",
    "\n",
    "$$\n",
    "   \\hat y_i = 1122.391 + 0.9978 x_i\n",
    "$$\n",
    "\n",
    "To estimate the variance of the errors we will use its unbiased estimator, the residual variance $s_R^2$,\n",
    "\n",
    "$$\n",
    "s_R^2 = \\frac{1}{n-2} \\sum_{i=1}^n e_i^2\n",
    "$$\n",
    "\n",
    "Replacing the value of $\\sum_i e_i^2$, we have\n",
    "\n",
    "$$\n",
    "s_R^2 = \\frac{29821}{17} = 1754.18\n",
    "$$\n",
    "\n",
    "If we did not have the value of $\\sum_i e_i^2$, we still can obtain the residual variance from\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\sum_i e_i^2 & = & \\sum_i (y_i - \\hat y_i)^2 = \\sum_i (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i)^2 \\\\\n",
    "& = & \\sum_i y_i^2 + n \\hat \\beta_0^2 + \\hat \\beta_1^2 \\sum_i x_i^2 - 2 \\hat \\beta_0 \\sum_i y_i - 2 \\hat \\beta_1 \\sum_i x_i y_i + 2 \\hat \\beta_0 \\hat \\beta_1 \\sum_i x_i \\\\\n",
    "& = & 59091545 + 19\\times 1122.391^2 + 0.9978^2 \\times 8073424 - 2 \\times 1122.391 \\times 19 \\times 1757.502 \\\\\n",
    "& & \\hbox{} - 2\\times 0.9978 \\times 21629441 + 2\\times 1122.391 \\times 0.9978 \\times 19 \\times 636.5 \\\\\n",
    "& = & 29821\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "the same value as before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93cc4fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## <span style=\"color:brown;\">Distributions for the parameter estimators</span>\n",
    "\n",
    "---\n",
    "\n",
    "The distributions of the estimators for the parameters of the regression model are obtained from the assumptions we have introduced on the model, and by considering that the values of the independent variable, $X$, are given. That is, when we derive a distribution for $\\hat \\beta_1$ we will be obtaining the distribution of $\\hat \\beta_1 | (\\underline{X} = \\{x_1,\\ldots , x_n\\}) = \\hat \\beta_1 | \\underline{X}$, where $\\underline{X}$ denotes the sample values of this independent variable, $\\underline{X} = \\{ X_1 , \\ldots , X_n \\}$. To simplify the notation, in all that follows we will omit this conditional assumption from the formulas.\n",
    "\n",
    "The knowledge and use of these functions and distributions is a basic prerequisite to conduct inference on these parameters and on the model in general. The distributions for the estimators of the three parameters are:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "T_{\\beta_1} & = & \\displaystyle \\frac{\\hat \\beta_1 - \\beta_1}{\\mbox{se}(\\hat \\beta_1)} \\sim t_{n-2} \\\\\n",
    "T_{\\beta_0} & = & \\displaystyle \\frac{\\hat \\beta_0 - \\beta_0}{\\mbox{se}(\\hat \\beta_0)} \\sim t_{n-2} \\\\\n",
    "T_{\\sigma^2} & = & \\displaystyle \\frac{(n-2)s_R^2}{\\sigma^2} \\sim \\chi^2_{n-2}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The standard errors for these statistics, indicated in the preceding formulas as $\\text{se} (\\hat \\beta_j)$, are given by\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\mbox{se}(\\hat \\beta_1) & = & \\displaystyle \\sqrt{\\frac{s_R^2}{(n-1)s_x^2}} \\\\\n",
    "\\mbox{se}(\\hat \\beta_0) & = & \\displaystyle \\sqrt{s_R^2\\left( \\frac{1}{n} + \\frac{\\bar x^2}{(n-1)s_x^2} \\right)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The derivation of these results can be found in [Appendix 3](#App4_3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67003266",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Standard errors for the parameter estimators\n",
    "\n",
    "se.beta.1 = sqrt(sR.2/((n.obs - 1)*s2.x))\n",
    "se.beta.0 = sqrt(sR.2*(1/n.obs + mn.x^2/((n.obs - 1)*s2.x)))\n",
    "\n",
    "val.0 <- round(c(hat.beta.0,hat.beta.1,sR.2,se.beta.0,se.beta.1,0),3)\n",
    "out.0 <- as.data.frame(matrix(val.0,3,2))\n",
    "out.0[3,2] = \" \"\n",
    "rownames(out.0) = c(\"beta 0\",\"beta 1\",\"Variance of errors\")\n",
    "colnames(out.0) = c(\"Estimates\",\"Std errors\")\n",
    "\n",
    "table_prnt(out.0,\"Parameter estimates and std errors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e1727e-4abf-47f5-ad10-8d300cc0c0a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:brown;\">Inference on the parameters of the linear regression model</span>\n",
    "\n",
    "Once we have identified the pivotal statistics and their distributions, we can conduct inference on these parameters. In particular, we can compute confidence intervals for any of the parameters. For a confidence level $1 - \\alpha$ these intervals are given by\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{CI}_{1-\\alpha} (\\beta_1) & = & \\displaystyle \\left[ \\hat \\beta_1 - t_{n-2,\\alpha/2}\\, \\text{se}(\\hat \\beta_1) \\; ; \\; \\hat \\beta_1 + t_{n-2,\\alpha/2}\\, \\text{se}(\\hat \\beta_1) \\right] \\\\ \n",
    "\\text{CI}_{1-\\alpha} (\\beta_0) & = & \\displaystyle \\left[ \\hat \\beta_0 - t_{n-2,\\alpha/2}\\, \\text{se}(\\hat \\beta_0) \\; ; \\; \\hat \\beta_0 + t_{n-2,\\alpha/2}\\, \\text{se}(\\hat \\beta_0) \\right] \\\\ \n",
    "\\text{CI}_{1-\\alpha} (\\sigma^2) & = & \\displaystyle \\left[ \\frac{(n-2)s_R^2}{\\chi^2_{n-2;\\alpha/2}} \\; ; \\; \\frac{(n-2)s_R^2}{\\chi^2_{n-2;1-\\alpha/2}} \\right]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We can also use this information to conduct hypothesis tests on the values of the population parameters and obtain p-values to measure the significance of these parameters, by applying the procedures described in Lesson 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97931e4f-2fdb-470f-bbc8-a0216f54f5c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### <span style=\"color:brown;\">Significance test for the slope</span>\n",
    "\n",
    "We now present a specially relevant test on $\\beta_1$, the slope of the model. This parameter is the most informative one for the simple linear regression model, as it provides direct evidence on the relationship between the values of $Y$ and $X$. In particular, if there is no linear relationship between $Y$ and $X$ then it should hold that $\\beta_1 = 0$.\n",
    "\n",
    "We often wish to test the significance of the model, that is, the existence of a significant linear relationship between the dependent and the independent variables, based on the value of $\\beta_1$. From the preceding comments, this significance test can be defined as\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "H_0 & : & \\beta_1 = 0 \\\\\n",
    "H_1 & : & \\beta_1 \\not= 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Based on the distribution of the estimator $\\hat \\beta_1$, we can compute the p-value for this test from the expression\n",
    "\n",
    "$$\n",
    "\\mbox{p-value} = 2 \\Pr \\left( T_{n-2} > \\frac{| \\hat \\beta_1 |}{\\text{se}(\\hat \\beta_1)} \\right) = 2 \\Pr \\left( T_{n-2} > \\frac{| \\hat \\beta_1 |}{\\displaystyle \\sqrt{\\frac{s_R^2}{(n-1)s_x^2}}} \\right) ,\n",
    "$$\n",
    "\n",
    "where $T_{n-2}$ denotes a random variable having a Student t distribution with $n-2$ degrees of freedom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a0833-acea-45bb-8eec-f3dae2e9b3c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### <span style=\"color:green;\">Questions</span>\n",
    "\n",
    "<span style=\"color:green\">Answer the following questions:</span>\n",
    "- <span style=\"color:green\">If the value of the test statistic for the slope of the regression line is smaller than its standard error, does it imply that there is no significant linear relationship between the variables? Why?</span>\n",
    "- <span style=\"color:green\">Assume that the values of $X$ and $Y$ are modified by intoducing linear transformations $X' = a + b X$, $Y' = c + d Y$, is it true that the significance of the linear relationship between $X'$ and $Y'$ is the same as that between $X$ and $Y$? If it were different, how would the values of the constants affect it?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b490fd-d9e9-40a4-a8b7-fa1e6ba8da9a",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue;\">Conducting a significance test for the slope</span>\n",
    "\n",
    "In the following cell we present the results corresponding to the linear regression model significance test for to the data we have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b444ad92-78e0-4f2f-bcdc-146e9dc3af2e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Building confidence intervals and hypothesis testing for model significance\n",
    "\n",
    "## This value can be modified\n",
    "\n",
    "conf.lvl.1 = 0.95\n",
    "\n",
    "## Parameter computations\n",
    "\n",
    "q.p = 0.5*(1 + conf.lvl.1)\n",
    "q.beta.1 = qt(q.p,n.obs-2,lower.tail=T)\n",
    "tst.stat = abs(hat.beta.1)/se.beta.1\n",
    "p.beta.1 = pt(tst.stat,n.obs-2,lower.tail=F)\n",
    "\n",
    "## Printouts for outputs\n",
    "\n",
    "val.0 <- c(sprintf('%7.2f',conf.lvl.1))\n",
    "out.0 <- as.data.frame(matrix(val.0,1,1))\n",
    "rownames(out.0) = c(\"Confidence level:\")\n",
    "colnames(out.0) = c(\"Value\")\n",
    "\n",
    "table_prnt(out.0,\"Conf/signif levels\")\n",
    "\n",
    "val.1 <- c(sprintf('[%8.4f;%8.4f ]',\n",
    "            hat.beta.1-q.beta.1*se.beta.1,hat.beta.1+q.beta.1*se.beta.1))\n",
    "out.1 <- as.data.frame(matrix(val.1,1,1))\n",
    "rownames(out.1) = c(\"Confidence interval:\")\n",
    "colnames(out.1) = c(\"Values\")\n",
    "\n",
    "table_prnt(out.1,\"Confidence interval beta1\")\n",
    "\n",
    "val.2 <- round(c(tst.stat,p.beta.1),4)\n",
    "out.2 <- as.data.frame(matrix(c(val.2),2,1))\n",
    "rownames(out.2) = c(\"Test statistic value:\",\"p value significance test:\")\n",
    "colnames(out.2) = c(\"p value\")\n",
    "\n",
    "table_prnt(out.2,\"Significance test beta1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76352609-7811-4f19-8faa-6f6ad33dd6eb",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style=\"color:red\">Exercise</span>\n",
    "\n",
    "*A fast-food company wants to evaluate the relationship between the number of ads in a social media app it purchases each week ($x$) and the profits in euros from its home-delivery sales during the week ($y$). From the observed data corresponding to a sample of 19 observations (weeks) we have the following information:*\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\bar x & = & 636.5 , \\quad \\bar y = 1757.502 \\\\\n",
    "\\sum_{i=1}^{19} x_i^2 & = & 8073424 , \\quad \\sum_{i=1}^{19} y_i^2 = 59091545 , \\quad \\sum_{i=1}^{19} x_i y_i = 21629441 \\\\\n",
    "\\sum_{i=1}^{19} e_i^2 & = & 29821\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- *Estimate the regression line for the weekly profits in terms of the number of social media ads. Compute an estimate for the variance of the errors in the regression model.*\n",
    "\n",
    "- *Compute a confidence interval at a 99% confidence level for the slope of the regression line.*\n",
    "\n",
    "- *Test the hypothesis that the slope of the regression line is different from zero at a significance level of 5%.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cd7db-5deb-43e2-8235-c58477b08082",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "##### <span style=\"color:red\">Exercise. Solution</span>\n",
    "\n",
    "We define our variables $X =$ \"Number of ads distributed in a week\" and $Y =$ \"Weekly profit in euros\"\n",
    "\n",
    "We have already obtained the estimated regression line as:\n",
    "\n",
    "$$\n",
    "   \\hat y_i = 1122.391 + 0.9978 x_i\n",
    "$$\n",
    "\n",
    "and the residual variance as\n",
    "\n",
    "$$\n",
    "s_R^2 = \\frac{29821}{17} = 1754.18\n",
    "$$\n",
    "\n",
    "To compute the confidence interval for the slope of the regression line for $1-\\alpha = 0.99$, we need the value of the quantile $t_{n-2;\\alpha/2} = t_{17;0.005} = 2.898$. We also have that\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "(n-1) s_x^2 & = & \\sum_i x_i^2 - n \\bar x^2 = 8073424 - 19 \\times 636.5^2 = 375911.2 \\\\\n",
    "\\mbox{se}(\\hat \\beta_1) & = & \\displaystyle \\sqrt{\\frac{s_R^2}{(n-1)s_x^2}} = \\sqrt{\\frac{1754.18}{375911.2}} = 0.06831\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We can now apply the formula for the confidence interval, and we obtain\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{CI}_{1-\\alpha} (\\beta_1) & = & \\displaystyle \\left[ \\hat \\beta_1 - t_{n-2,\\alpha/2}\\, \\text{se}(\\hat \\beta_1) \\; ; \\; \\hat \\beta_1 + t_{n-2,\\alpha/2}\\, \\text{se}(\\hat \\beta_1) \\right] \\\\\n",
    "& = & \\displaystyle \\left[ 0.9978 - 2.898\\times 0.06831 \\; ; \\; 0.9978 + 2.898\\times 0.06831 \\right] = \\left[ 0.7998 \\; ; \\; 1.1958 \\right]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For the significance test, we define it as\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "H_0 & : & \\beta_1 = 0 \\\\\n",
    "H_1 & : & \\beta_1 \\not= 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Its test statistic is given by\n",
    "\n",
    "$$\n",
    "T_{\\beta_1} = \\frac{\\hat \\beta_1 - \\beta_1}{\\mbox{se}(\\hat \\beta_1)} \\sim t_{n-2} \n",
    "$$\n",
    "\n",
    "and its value under the null hypothesis is\n",
    "\n",
    "$$\n",
    "t_0 = \\frac{\\hat \\beta_1}{\\mbox{se}(\\hat \\beta_1)} = \\frac{0.9978}{0.06831} = 14.607\n",
    "$$\n",
    "\n",
    "This value is huge, much larger than the critical value, $t_{n-2;\\alpha/2} = t_{17;0.025} = 2.110$. As $t_0$ lies in the critical region for the test, we conclude that the model is significant, that is, there exists a significant linear relationship between these two variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24385ad5-7031-4cbc-999b-eeec8d326a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numerical calculations for the exercise\n",
    "\n",
    "# Data for the exercise\n",
    "\n",
    "n.obs = 19\n",
    "x.bar = 636.5\n",
    "y.bar = 1757.502\n",
    "x.sum2 = 8073424\n",
    "xy.sum = 21629441\n",
    "e.sum2 = 29821\n",
    "\n",
    "# Some estimates\n",
    "\n",
    "s.x2 = (x.sum2 - n.obs*x.bar^2)/(n.obs-1)\n",
    "s.xy = (xy.sum - n.obs*x.bar*y.bar)/(n.obs-1)\n",
    "hat.beta1 = s.xy/s.x2\n",
    "s.R2 = e.sum2/(n.obs-2)\n",
    "se.beta1 = sqrt(s.R2/((n.obs-1)*s.x2))\n",
    "\n",
    "# Confidence interval\n",
    "\n",
    "conf.lvl = 0.99\n",
    "\n",
    "quant.val = qt(0.5 + conf.lvl/2,n.obs-2)\n",
    "ci.lv = hat.beta1 - quant.val*se.beta1\n",
    "ci.uv = hat.beta1 + quant.val*se.beta1\n",
    "\n",
    "cat(sprintf(\"\\nConfidence level          : %8.4f\\n\",conf.lvl))\n",
    "cat(sprintf(\"Confidence interval       : [ %8.4f ; %8.4f ]\\n\",ci.lv,ci.uv))\n",
    "\n",
    "# Significance test\n",
    "\n",
    "sig.lvl = 0.05\n",
    "cat(sprintf(\"\\nSignificance level                : %10.4f\\n\",conf.lvl))\n",
    "\n",
    "t.0 = hat.beta1/se.beta1\n",
    "cat(sprintf(\"Value of the statistic under H_0  : %10.4f\\n\",t.0))\n",
    "\n",
    "crit.v = qt(sig.lvl/2,n.obs-2,lower.tail=FALSE)\n",
    "cat(sprintf(\"Critical value for the test       : %10.4f\\n\",crit.v))\n",
    "\n",
    "p.val = 2*pt(t.0,n.obs-2,lower.tail=FALSE)\n",
    "cat(sprintf(\"P-value for the test              : %10.4f\\n\",p.val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0a76f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### <span style=\"color:blue\">Computing regression estimates using R functions</span>\n",
    "\n",
    "We now describe how to conduct the preceding calculations using functions available in R.\n",
    "\n",
    "Several functions are able to compute the values of the estimates for the parameters in the model, and provide other general results corresponding to the fitting of a regression model.\n",
    "\n",
    "A widely used option is the function <span style=\"color:blue;font-family:monospace;font-size:90%;\">lm</span> (where *lm* stands for Linear Model). For general linear regression models, this function computes and returns the estimates of the coefficients, the standard errors, the p-values of the significance tests for the parameters being equal to zero, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ef0ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Using R to estimate linear regression models\n",
    "\n",
    "## Results obtained using R functions\n",
    "\n",
    "lr.xy = lm(Y ~ X, data = data.sel)\n",
    "lr.sum = summary(lr.xy)\n",
    "lr.sum.val = lapply(lr.sum[4],round,5)\n",
    "\n",
    "table_prnt(lr.sum.val,\"Parameter values\")\n",
    "\n",
    "lr.sum.2 = round(as.data.frame(lr.sum[8:9]),3)\n",
    "colnames(lr.sum.2) = c(\"R squared\",\"Adj R sq\")\n",
    "\n",
    "lr.sum.3 = round(as.data.frame(lr.sum[10]),3)\n",
    "colnames(lr.sum.3) = c(\"F statistic\")\n",
    "rownames(lr.sum.3) = c(\"Value\",\"df num\",\"df den\")\n",
    "\n",
    "table_prnt(list(lr.sum.2,lr.sum.3),\"Coeff of determination and global significance\")\n",
    "\n",
    "lr.sum.h = huxreg(\"Values\" = lr.xy, number_format = \"%10.4f\")\n",
    "lr.sum.h = lr.sum.h %>% set_caption(\"Parameter estimates using huxreg\")\n",
    "colnames(lr.sum.h) <- c(\"Parameters\",\"Values\")\n",
    "rownames(lr.sum.h) <- c(0:9)\n",
    "lr.sum.h <- lr.sum.h[c(-1,-10),]\n",
    "cat(\"\\n------\\n\")\n",
    "print(lr.sum.h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ae9bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## <span style=\"color:brown;\">Mean responses and forecasts</span>\n",
    "\n",
    "---\n",
    "\n",
    "In the preceding sections we have described how to estimate the parameters of our simple linear regression model, as well as how to conduct a test to determine if the linear relationship we are considering is significant. Once we have the information from these steps, we may proceed to use the model to obtain information for the dependent variable $Y$, assuming that we have some additional values of interest for the independent variable, not included in our original sample.\n",
    "\n",
    "This problem is known as that of obtaining a <span style=\"color:brown;\">forecast/mean response</span> estimate for the value of $Y$ corresponding to a given value of the independent variable, $x = x_0$.\n",
    "\n",
    "There are two types of relevant information that we would like to obtain from the model:\n",
    "\n",
    "- A <span style=\"color:brown;\">forecast</span> estimate, that is, a value that would approximate as best as possible, given our information and the fitted model, the value we would observe for <span style=\"color:brown;\">*one instance of the dependent variable*,</span> when the independent variable takes a given value.\n",
    "\n",
    "- A <span style=\"color:brown;\">mean response</span> estimate, that is, a value that would approximate as best as possible the <span style=\"color:brown;\">*average value of all the occurrences of the dependent variable*,</span> when the independent variable takes the same given value for a very large number of observations of $Y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373b507-516e-437a-a733-26e8bd97ddc2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:brown;\">Inference on mean responses and forecasts</span>\n",
    "\n",
    "Following our comments in Lesson 1, to estimate the values of the (population) forecasts/mean responses for a given value of $X$ we will define point estimators with good properties for these values. Then, we will consider how to obtain confidence intervals for them.\n",
    "\n",
    "#### <span style=\"color:brown\">Point estimates</span> \n",
    "\n",
    "We define our point estimate by replacing in our definition of the quantities of interest: i) the independent variable $X$ with its known value $x_0$, ii) the unknown parameters in the linear regression model by their least squares estimates and iii) the error term $U$ by its expected value, equal to zero.\n",
    "\n",
    "For both of the preceding cases we obtain the same point estimator, given by\n",
    "\n",
    "$$\n",
    "\\hat Y_0 = \\hat \\beta_0 + \\hat \\beta_1 x_0 .\n",
    "$$\n",
    "\n",
    "This point estimator is unbiased in both cases, having expected value $E[\\hat Y_0] = \\beta_0 + \\beta_1 x_0 \\equiv y_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecca662-9ac5-4dcd-b592-530383d215dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:brown\">Confidence intervals</span>\n",
    "\n",
    "To obtain confidence intervals for the mean response and the forecast corresponding to the given value $X = x_0$, we need to identify the estimators to use in these cases, as well as their distributions.\n",
    "  \n",
    "- For the <span style=\"color:brown\">mean response</span> estimator we have\n",
    "\n",
    "$$\n",
    "T_{mr} = \\frac{\\hat Y_0 - y_0}{\\text{se}_{mr} (y_0)} \\sim t_{n-2}\n",
    "$$\n",
    "\n",
    "- For the <span style=\"color:brown\">forecast</span> estimator we have that\n",
    "\n",
    "$$\n",
    "T_f = \\frac{\\hat Y_0 - y_0}{\\text{se}_f (y_0)} \\sim t_{n-2}\n",
    "$$\n",
    "\n",
    "The only differences between these two cases are their standard errors, taking values:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\mbox{se}_{mr} (y_0) & = & \\displaystyle \\sqrt{s_R^2\\left( \\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{(n-1)s_x^2} \\right)} \\\\\n",
    "\\mbox{se}_{f} (y_0) & = & \\displaystyle \\sqrt{s_R^2\\left( 1 + \\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{(n-1)s_x^2} \\right)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The standard error for the forecast estimator is larger than the one for the mean response, and it does not go to zero when $n \\rightarrow \\infty$.\n",
    "\n",
    "Using this information, we can compute confidence intervals or conduct hypothesis testing on the values of the forecasts/mean responses. For example, given a confidence level $1-\\alpha$, the corresponding confidence intervals can be obtained using the formulas\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{CI}_{mr,1-\\alpha} (y_0) & = & \\left[ \\hat y_0 - t_{n-2;\\alpha/2}\\, \\mbox{se}_{mr} (y_0) \\, ; \\, \\hat y_0 + t_{n-2;\\alpha/2}\\, \\mbox{se}_{mr} (y_0) \\right] \\\\\n",
    "\\text{CI}_{f,1-\\alpha} (y_0) & = & \\left[ \\hat y_0 - t_{n-2;\\alpha/2}\\, \\mbox{se}_f (y_0) \\, ; \\, \\hat y_0 + t_{n-2;\\alpha/2}\\, \\mbox{se}_f (y_0) \\right]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The derivation of the distributions for the mean response and forecast estimators can be found in [Appendix 4](#App4_4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c2cfd-ea32-4e2e-83c6-104dcaf8744f",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style=\"color:red\">Exercise</span>\n",
    "\n",
    "*A fast-food company wants to evaluate the relationship between the number of ads in a social media app it purchases each week ($x$) and the profits in euros from its home-delivery sales during the week ($y$). From the observed data corresponding to a sample of 19 observations (weeks) we have the following information:*\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\bar x & = & 636.5 , \\quad \\bar y = 1757.502 \\\\\n",
    "\\sum_{i=1}^{19} x_i^2 & = & 8073424 , \\quad \\sum_{i=1}^{19} y_i^2 = 59091545 , \\quad \\sum_{i=1}^{19} x_i y_i = 21629441 \\\\\n",
    "\\sum_{i=1}^{19} e_i^2 & = & 29821\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- *Estimate the regression line for the weekly profits in terms of the number of social media ads. Compute an estimate for the variance of the errors in the regression model.*\n",
    "\n",
    "- *Estimate the value of the expected weekly profits in euros for those weeks when 550 ads were purchased. Compute a confidence interval at a confidence level of 95% for this estimation.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e142e48-c350-4b9c-aa70-e109dd006ba0",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "##### <span style=\"color:red\">Exercise. Solution</span>\n",
    "\n",
    "We define our variables $X =$ \"Number of ads distributed in a week\" and $Y =$ \"Weekly profit in euros\"\n",
    "\n",
    "We have already obtained the estimated regression line as:\n",
    "\n",
    "$$\n",
    "\\hat y_i = 1122.391 + 0.9978 x_i\n",
    "$$\n",
    "\n",
    "and the residual variance as\n",
    "\n",
    "$$\n",
    "s_R^2 = \\frac{29821}{17} = 1754.18\n",
    "$$\n",
    "\n",
    "From our preceding results, we also have that\n",
    "\n",
    "$$\n",
    "(n-1) s_x^2 = 375911.2\n",
    "$$\n",
    "\n",
    "The requested point estimate for $x_0 = 550$ is given by\n",
    "\n",
    "$$\n",
    "\\hat y_0 = 1122.391 + 0.9978 \\times 550 = 1671.18\n",
    "$$\n",
    "\n",
    "The confidence interval for the mean response (the average weekly profits) will be defined in terms of its standard error,\n",
    "\n",
    "$$\n",
    "\\mbox{se}_{mr} (y_0) = \\sqrt{s_R^2\\left( \\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{(n-1)s_x^2} \\right)} = \\sqrt{1754.18\\left( \\frac{1}{19} + \\frac{(550 - 636.5)^2}{375911.2} \\right)} = 11.2801\n",
    "$$\n",
    "\n",
    "and using $t_{n-2;\\alpha/2} = t_{17;0.025} = 2.110$, we obtain\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{CI}_{mr,1-\\alpha} (y_0) & = & \\left[ \\hat y_0 - t_{n-2;\\alpha/2}\\, \\mbox{se}_{mr} (y_0) \\, ; \\, \\hat y_0 + t_{n-2;\\alpha/2}\\, \\mbox{se}_{mr} (y_0) \\right] \\\\\n",
    "& = & \\displaystyle \\left[ 1671.18 - 2.110\\times 11.2801 \\; ; \\; 1671.18 + 2.110\\times 11.2801 \\right] \\\\\n",
    "& = & \\left[ 1647.38 \\; ; \\; 1694.98 \\right]\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f118acb-bbb6-4e4a-b9e9-096b886b2c30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "#### <span style=\"color:blue\">Computing mean responses and forecasts using R</span>\n",
    "\n",
    "In the following cell we obtain the values of point estimates and confidence intervals for particular cases, using the data from our fitted linear regression model. The value for $x_0$ used in the mean response/forecast calculation can be modified by assigning a different value to the variable <span style=\"color:blue;font-family:monospace;font-size:90%;\">x.0</span> in the following cell.\n",
    "\n",
    "We also show some plots to illustrate the impact of the value $x_0$ on the confidence intervals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d84a37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Mean responses and forecasts\n",
    "\n",
    "## These values can be modified\n",
    "\n",
    "x.0 = 7.5\n",
    "conf.lvl.2 = 0.95\n",
    "\n",
    "## Computation of parameter values\n",
    "\n",
    "q.p.2 = 0.5*(1 + conf.lvl.2)\n",
    "q.hat.y.0 = qt(q.p.2,n.obs-2,lower.tail=T)\n",
    "hat.y.0 = hat.beta.0 + hat.beta.1*x.0\n",
    "\n",
    "se.hat.y.0.mr = sqrt(sR.2*(1/n.obs + (mn.x - x.0)^2/(n.obs-1)*s2.x))\n",
    "se.hat.y.0.fc = sqrt(sR.2*(1 + 1/n.obs + (mn.x - x.0)^2/(n.obs-1)*s2.x))\n",
    "\n",
    "## Printing the estimates\n",
    "\n",
    "val.0 <- round(c(x.0,hat.y.0),3)\n",
    "out.0 <- as.data.frame(matrix(val.0,length(val.0),1))\n",
    "rownames(out.0) = c(\"Value of x0:\",\"Point estimate for the response:\")\n",
    "colnames(out.0) = c(\"Values\")\n",
    "\n",
    "table_prnt(out.0,\"Point estimate\")\n",
    "\n",
    "val.2 = c(sprintf('%8.2f',conf.lvl.2),sprintf('%8.3f',se.hat.y.0.mr),sprintf('%8.3f',se.hat.y.0.fc),\n",
    "          sprintf('[%8.3f;%8.3f ]',hat.y.0-q.hat.y.0*se.hat.y.0.mr,hat.y.0+q.hat.y.0*se.hat.y.0.mr),\n",
    "          sprintf('[%8.3f;%8.3f ]',hat.y.0-q.hat.y.0*se.hat.y.0.fc,hat.y.0+q.hat.y.0*se.hat.y.0.fc))\n",
    "out.2 = as.data.frame(matrix(val.2,5,1))\n",
    "rownames(out.2) = c(\"Selected confidence level:\",\"Standard error mean response:\",\"Standard error forecast:\",\n",
    "                    \"Confidence interval for mean response:\",\"Confidence interval for forecast:\")\n",
    "colnames(out.2) = c(\"Values\")\n",
    "\n",
    "table_prnt(out.2,\"CIs mean response and forecast\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7683e-6797-4ccd-908e-04719cbdc4a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "#### <span style=\"color:blue\">Plotting mean responses and forecasts using R (i)</span>\n",
    "\n",
    "In the following cell we show the confidence intervals for a set of values of $x = x_0$, both in the case of mean responses, in red, and forecasts, in green.\n",
    "\n",
    "Note the different sizes of the intervals and their dependence on the value of $x_0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818915e-fb8d-4dd8-b2eb-be85b90a0f66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plots for mean responses and forecasts\n",
    "\n",
    "## These values can be modified\n",
    "\n",
    "qt.x = c(0.5,0.75,1)\n",
    "conf.lvl = 0.95\n",
    "\n",
    "## Computation of parameter values\n",
    "\n",
    "s.x = sqrt(s2.x)\n",
    "x.0m = mn.x + qt.x*s.x\n",
    "x.0f = x.0m + 0.05*s.x\n",
    "q.p.2 = 0.5*(1 + conf.lvl)\n",
    "q.hat.y0 = qt(q.p.2,n.obs-2,lower.tail=T)\n",
    "hat.y0m = hat.beta.0 + hat.beta.1*x.0m\n",
    "hat.y0f = hat.beta.0 + hat.beta.1*x.0f\n",
    "\n",
    "se.hat.y0m = sqrt(sR.2*(1/n.obs + (mn.x - x.0m)^2/(n.obs-1)*s2.x))\n",
    "se.hat.y0f = sqrt(sR.2*(1 + 1/n.obs + (mn.x - x.0f)^2/(n.obs-1)*s2.x))\n",
    "\n",
    "## Plotting results\n",
    "\n",
    "plt.sct.0 = data.sel %>% ggplot() + geom_point(aes(x=X,y=Y)) +\n",
    "  geom_line(aes(x=X,y=Yest),color=\"blue\",linewidth=0.75) +\n",
    "  ggtitle(sprintf(\"Forecast and Mean response CIs %s vs %s\",c.names[1],c.names[2])) +\n",
    "  labs(y = c.names[2], x = c.names[1]) +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "\n",
    "y.li.m = hat.y0m - q.hat.y0*se.hat.y0m\n",
    "y.ls.m = hat.y0m + q.hat.y0*se.hat.y0m\n",
    "y.li.f = hat.y0f - q.hat.y0*se.hat.y0f\n",
    "y.ls.f = hat.y0f + q.hat.y0*se.hat.y0f\n",
    "ci.df = data.frame(xm = x.0m, xf = x.0f, hym = hat.y0m, hyf = hat.y0f, lim = y.li.m, lsm = y.ls.m, lif = y.li.f, lsf = y.ls.f)\n",
    "\n",
    "plt.sct.1 = plt.sct.0 + geom_segment(data=ci.df,aes(x=xm,y=lim,xend=xm,yend=lsm),\n",
    "                                     inherit.aes=FALSE,color=\"red\",linewidth=0.75) +\n",
    "      geom_segment(data=ci.df,aes(x=xf,y=lif,xend=xf,yend=lsf),inherit.aes=FALSE,color=\"green\",linewidth=0.75) +\n",
    "      geom_point(data=ci.df,aes(x=xm,y=hym),inherit.aes=FALSE,color=\"red\") +\n",
    "      geom_point(data=ci.df,aes(x=xf,y=hyf),inherit.aes=FALSE,color=\"green\")\n",
    "\n",
    "plot(plt.sct.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edda56d-404d-4258-9f1c-c7c0b1f8a1ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "#### <span style=\"color:blue\">Plotting mean responses and forecasts using R (ii)</span>\n",
    "\n",
    "In the following cell we show the confidence intervals associated to all possible values of $x$ for mean responses and forecasts, as grey bands on the scatterplot for the data. We present these results for different sample sizes, to illustrate the impact of $n$ on the length of the intervals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe69723-4453-4b08-ac3b-09280ea25238",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plots for mean responses and forecasts (ii)\n",
    "\n",
    "## These values can be modified\n",
    "\n",
    "conf.lvl = 0.95\n",
    "n.smp.1 = 20\n",
    "n.smp.2 = 50\n",
    "\n",
    "## Computation of parameter values\n",
    "\n",
    "n.obs = nrow(data.sel)\n",
    "q.cl = 0.5*(1 + conf.lvl)\n",
    "qt.y1 = qt(q.cl,n.smp.1-2,lower.tail=T)\n",
    "qt.y2 = qt(q.cl,n.smp.2-2,lower.tail=T)\n",
    "\n",
    "## Generation of the samples\n",
    "\n",
    "ix.sel.1 = sample(n.obs,n.smp.1)\n",
    "data.sel.1 = data.sel[ix.sel.1,]\n",
    "ix.sel.2 = sample(n.obs,n.smp.2)\n",
    "data.sel.2 = data.sel[ix.sel.2,]\n",
    "\n",
    "x.mn = min(c(data.sel.1$X,data.sel.2$X))\n",
    "x.mx = max(c(data.sel.1$X,data.sel.2$X))\n",
    "x.val = seq(x.mn,x.mx,length.out = 100)\n",
    "\n",
    "## Computation of sample statistics\n",
    "\n",
    "x.mn.1 = mean(data.sel.1$X)\n",
    "x.sd.1 = sd(data.sel.1$X)\n",
    "y.mn.1 = mean(data.sel.1$Y)\n",
    "y.sd.1 = sd(data.sel.1$Y)\n",
    "x.mn.2 = mean(data.sel.2$X)\n",
    "x.sd.2 = sd(data.sel.2$X)\n",
    "y.mn.2 = mean(data.sel.2$Y)\n",
    "y.sd.2 = sd(data.sel.2$Y)\n",
    "\n",
    "## Estimation of the models\n",
    "\n",
    "hat.b1.1 = cov(data.sel.1$X,data.sel.1$Y)/x.sd.1^2\n",
    "hat.b0.1 = y.mn.1 - hat.b1.1*x.mn.1\n",
    "hat.b1.2 = cov(data.sel.2$X,data.sel.2$Y)/x.sd.2^2\n",
    "hat.b0.2 = y.mn.2 - hat.b1.2*x.mn.2\n",
    "\n",
    "data.sel.1$Yest = hat.b0.1 + hat.b1.1*data.sel.1$X\n",
    "data.sel.2$Yest = hat.b0.2 + hat.b1.2*data.sel.2$X\n",
    "\n",
    "hat.y1 = hat.b0.1 + hat.b1.1*x.val\n",
    "hat.y2 = hat.b0.2 + hat.b1.2*x.val\n",
    "\n",
    "res.1 = data.sel.1$Y - data.sel.1$Yest\n",
    "res.2 = data.sel.2$Y - data.sel.2$Yest\n",
    "\n",
    "sR2.1 = sum(res.1^2)/(n.smp.1-2)\n",
    "sR2.2 = sum(res.2^2)/(n.smp.2-2)\n",
    "\n",
    "## Standard errors in forecasting\n",
    "\n",
    "se.ym.1 = sqrt(sR2.1*(1/n.smp.1 + (x.mn.1 - x.val)^2/(n.smp.1-1)*x.sd.1^2))\n",
    "se.yf.1 = sqrt(sR2.1*(1 + 1/n.smp.1 + (x.mn.1 - x.val)^2/(n.smp.1-1)*x.sd.1^2))\n",
    "se.ym.2 = sqrt(sR2.2*(1/n.smp.2 + (x.mn.2 - x.val)^2/(n.smp.2-1)*x.sd.2^2))\n",
    "se.yf.2 = sqrt(sR2.2*(1 + 1/n.smp.2 + (x.mn.2 - x.val)^2/(n.smp.2-1)*x.sd.2^2))\n",
    "\n",
    "y.li.m = hat.y1 - qt.y1*se.ym.1\n",
    "y.ls.m = hat.y1 + qt.y1*se.ym.1\n",
    "y.li.f = hat.y1 - qt.y1*se.yf.1\n",
    "y.ls.f = hat.y1 + qt.y1*se.yf.1\n",
    "vl.u1 = max(y.ls.f)\n",
    "vl.l1 = min(y.li.f)\n",
    "ci.df.1 = data.frame(x = x.val, hy = hat.y1, lim = y.li.m, lsm = y.ls.m, lif = y.li.f, lsf = y.ls.f)\n",
    "y.li.m = hat.y2 - qt.y2*se.ym.2\n",
    "y.ls.m = hat.y2 + qt.y2*se.ym.2\n",
    "y.li.f = hat.y2 - qt.y2*se.yf.2\n",
    "y.ls.f = hat.y2 + qt.y2*se.yf.2\n",
    "vl.u2 = max(y.ls.f)\n",
    "vl.l2 = min(y.li.f)\n",
    "\n",
    "vl.u = max(vl.u1,vl.u2)\n",
    "vl.l = min(vl.l1,vl.l2)\n",
    "vlim.u = vl.u + 0.05*(vl.u - vl.l)\n",
    "vlim.l = vl.l - 0.05*(vl.u - vl.l)\n",
    "ci.df.2 = data.frame(x = x.val, hy = hat.y2, lim = y.li.m, lsm = y.ls.m, lif = y.li.f, lsf = y.ls.f)\n",
    "\n",
    "## Plotting results\n",
    "\n",
    "plt.sct.1a = data.sel.1 %>% ggplot() + geom_point(aes(x=X,y=Y)) +\n",
    "  geom_line(aes(x=X,y=Yest),color=\"blue\",size=0.75) +\n",
    "  ggtitle(sprintf(\"Forecast and Mean response CIs sample size %3.0f\",n.smp.1)) +\n",
    "  xlim(x.mn,x.mx) + ylim(vlim.l,vlim.u) +\n",
    "  labs(y = c.names[2], x = c.names[1]) +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "plt.sct.1b = plt.sct.1a + geom_ribbon(data=ci.df.1,aes(x=x,ymin=lim,ymax=lsm),inherit.aes=FALSE,alpha=0.5) +\n",
    "                   geom_ribbon(data=ci.df.1,aes(x=x,ymin=lif,ymax=lsf),inherit.aes=FALSE,alpha=0.25)\n",
    "\n",
    "plt.sct.2a = data.sel.2 %>% ggplot() + geom_point(aes(x=X,y=Y)) +\n",
    "  geom_line(aes(x=X,y=Yest),color=\"blue\",size=0.75) +\n",
    "  ggtitle(sprintf(\"Forecast and Mean response CIs sample size %3.0f\",n.smp.2)) +\n",
    "  xlim(x.mn,x.mx) + ylim(vlim.l,vlim.u) +\n",
    "  labs(y = c.names[2], x = c.names[1]) +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "plt.sct.2b = plt.sct.2a + geom_ribbon(data=ci.df.2,aes(x=x,ymin=lim,ymax=lsm),inherit.aes=FALSE,alpha=0.5) +\n",
    "                   geom_ribbon(data=ci.df.2,aes(x=x,ymin=lif,ymax=lsf),inherit.aes=FALSE,alpha=0.25)\n",
    "\n",
    "suppressWarnings(grid.arrange(plt.sct.1b,plt.sct.2b,nrow = 2,\n",
    "                              top=textGrob(sprintf(\"Forecasting errors for different sample sizes\",s.pref$title),\n",
    "                                           gp=gpar(fontsize=15,col=\"blue\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f16924-bc15-49b9-91c8-79f3a6d26a19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:brown;\">Assessing the quality of the linear regression model</span>\n",
    "\n",
    "---\n",
    "\n",
    "In this final part of the lesson, we introduce some measures to evaluate the quality of the approximations provided by the fitted linear regression model, obtained from a given sample.\n",
    "\n",
    "Our approach is that we wish to use this model to improve our knowledge of the values of the dependent variable $Y$, by efficiently incorporating the information provided by the values of the independent variable $X$. We will measure this improvement through the variability in the estimates for $Y$, with and without the information in $X$. We associate the quality of our model to the magnitude of the reduction in this variability of the estimates for our dependent variable, which we might achieve by using the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866aedc3-3c4a-4cda-ae9d-8041463744c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### <span style=\"color:brown;\">The correlation coefficient</span>\n",
    "\n",
    "From the value of the covariance between the two variables, which we need for our least squares parameter estimates, we can compute the value of the correlation coefficient for these variables. It was already commented in Statistics I how this coefficient provides relevant information either to support or to doubt the existence of a linear relationship between these variables.\n",
    "\n",
    "Its value is defined as\n",
    "\n",
    "$$\n",
    "\\mbox{cor}(x,y) = \\frac{\\mbox{cov}(x,y)}{s_x s_y} \\in [-1,1]\n",
    "$$\n",
    "\n",
    "If the value of this coefficient were close to $+1$ or $-1$, it would be an indication of a clear linear relationship between the variables, while values close to 0 would be an indication of the absence of any such relationship.\n",
    "\n",
    "The following cell shows its value for the data we have been studying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da017e-aa79-4bad-b9d4-c9d1d46d7bee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation coefficient\n",
    "\n",
    "cor.xy = cov.xy/sqrt(s2.x*s2.y)\n",
    "\n",
    "rn.1 <- sprintf('Correlation coef of %s %s',c.names[1],c.names[2])\n",
    "val.0 <- round(c(cor.xy),3)\n",
    "out.0 <- as.data.frame(matrix(val.0,length(val.0),1))\n",
    "colnames(out.0) = c(\"Value\")\n",
    "rownames(out.0) = c(rn.1)\n",
    "\n",
    "table_prnt(out.0,\"Correlation coef\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7b7036-1a3b-4bbb-a1a9-673722a4f274",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "#### <span style=\"color:blue;\">Graphical interpretation of the correlation coefficient</span>\n",
    "\n",
    "To illustrate further the relationship between the correlation coefficient and the characteristics of the linear regression model, the following cell presents a scatterplot obtained for simulated data, where the value of the correlation coefficient has been specified in advance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f915ec6-302e-4f72-8f67-bf258caa06de",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Plot for the correlation coefficient\n",
    "\n",
    "## Modify this value and run the cell\n",
    "\n",
    "xy.cor = -0.75\n",
    "\n",
    "## Generate random data\n",
    "\n",
    "n.obs = 100\n",
    "x.var = 1\n",
    "y.var = 0.6\n",
    "\n",
    "v.obs.0 = matrix(rnorm(2*n.obs),n.obs,2)\n",
    "v.obs.0 = scale(v.obs.0)\n",
    "v.obs.0[,2] = v.obs.0[,2] - cov(v.obs.0)[1,2]*v.obs.0[,1]\n",
    "xy.cov = xy.cor*sqrt(x.var*y.var)\n",
    "xy.S = matrix(c(x.var,xy.cov,xy.cov,y.var),2,2)\n",
    "xy.S.eig = eigen(xy.S)\n",
    "xy.sqS = xy.S.eig$vectors %*% diag(sqrt(c(xy.S.eig$values))) %*% t(xy.S.eig$vectors) \n",
    "v.obs = v.obs.0 %*% xy.sqS\n",
    "v.obs = scale(v.obs)\n",
    "df.obs = data.frame(X=v.obs[,1],Y=v.obs[,2])\n",
    "\n",
    "## Regression line\n",
    "\n",
    "sim.S = cov(df.obs)\n",
    "sim.mn = colMeans(df.obs)\n",
    "beta.1.sim = sim.S[1,2]/sim.S[1,1]\n",
    "beta.0.sim = sim.mn[2] - beta.1.sim*sim.mn[1]\n",
    "df.obs$Yest = beta.0.sim + beta.1.sim*df.obs$X\n",
    "\n",
    "## Show the resulting scatterplot\n",
    "\n",
    "xy.cor.e = cor(df.obs$X,df.obs$Y)\n",
    "plt.scat.a = df.obs %>% ggplot() + geom_point(aes(x=X,y=Y)) +\n",
    "  ggtitle(sprintf(\"Scatterplot normal data. Correlation coef %5.2f\",xy.cor.e)) +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "plt.scat.b = plt.scat.a + geom_line(aes(x=X,y=Yest),color=\"blue\",size=0.75)\n",
    "\n",
    "suppressWarnings(grid.arrange(plt.scat.a,plt.scat.b,nrow = 2,\n",
    "                              top=textGrob(sprintf(\"Scatterplots for simulated data\",s.pref$title),\n",
    "                                           gp=gpar(fontsize=15,col=\"blue\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b47e93-00d8-41be-8fc4-40a78e48f167",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### <span style=\"color:brown;\">Other quality measures</span>\n",
    "\n",
    "We have illustrated that when the value of the correlation coefficient is close to $\\pm 1$, we can expect a close linear relationship between the two variables, while values close to 0 indicate the absence of a linear relationship. But a difficulty with this measure is that it is not clear where to set a limit to separate good models from those that are not as useful to explain $Y$.\n",
    "\n",
    "To conduct inference, we usually prefer measures that have a known distribution associated to it, which can be used to associate probabilities to specific situations and to conduct hypothesis tests. In that sense, the test statistic we introduced for the significance test of the linear regression model,\n",
    "\n",
    "$$\n",
    "T_{\\beta_1} = \\displaystyle \\frac{\\hat \\beta_1}{\\mbox{se}(\\hat \\beta_1)} \\sim t_{n-2}\n",
    "$$\n",
    "\n",
    "provides an alternative measure of the quality of the model, based on a different scale, but with the advantage of following a known distribution. We have already seen how to define reasonable limits for acceptable models vs.\\ models that do not provide information on the relationship of interest, based on this measure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ee896-e3da-4ea4-8f77-ffd004245894",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### <span style=\"color:brown;\">Measures based on variability reduction</span>\n",
    "\n",
    "There is another requirement we would like to impose on our quality measures: the existence of some intuitive interpretation for the values of the selected measure. For example, this interpretation is reasonably clear for the correlation coefficient, although it is not so clear for the value of the test statistic for the significance test.\n",
    "\n",
    "In what follows we introduce some additional measures used often in practice, based on the reduction in the variability in the observed values of $Y$ that we could achieve by using additional information from the value the independent variable $X$. These measures also have intuitive interpretations.\n",
    "\n",
    "We start with some observations about variability measures:\n",
    "\n",
    "- When the information available for the variable $Y$ is based only on the values of a given sample, without taking into account any other information, a measure of its variability (the total variability) is given by the value of its quasivariance, $s_y^2$. \n",
    "- But if we have a paired sample of values from both the independent variable $X$ and from $Y$, and we use the values of $x_i$ to obtain an approximation for the value of $y_i$ based on the linear regression model relating the two variables, $\\hat y_i$, the variability that is left unexplained in the values of $Y$ corresponds to the residuals of the model. And its estimated variance is given by the residual variance, $s_R^2$.\n",
    "\n",
    "By the variability left unexplained we mean the part of the value of $Y$ that is different from the value predicted by the model. That is, we take the value $\\hat y_i = \\hat \\beta_0 + \\hat \\beta_1 x_i$ as known given the value of $X$ and the linear regression model. The part of the value of $y_i$ that we are not able to predict is the difference between $y_i$ and $\\hat y_i$, that is, $y_i - \\hat y_i = e_i$.\n",
    "\n",
    "In the following cell we present two plots comparing the total variability in $Y$ with the variability left unexplained after using the regression model, the residual variability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298f554-d23d-46bd-afdc-c1d8e3c0e55c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plots for variance reduction\n",
    "\n",
    "## Plotting results\n",
    "\n",
    "vlim.l = min(df.obs$Y)\n",
    "vlim.u = max(df.obs$Y)\n",
    "\n",
    "n.obs = nrow(df.obs)\n",
    "y.mn = mean(df.obs$Y)\n",
    "y.sd = sd(df.obs$Y)\n",
    "x.sd = sd(df.obs$X)\n",
    "beta.1.sel = cov(df.obs$X,df.obs$Y)/var(df.obs$X)\n",
    "beta.0.sel = y.mn - beta.1.sel*mean(df.obs$X)\n",
    "df.obs$Yest = beta.0.sel + beta.1.sel*df.obs$X\n",
    "df.obs$Res = df.obs$Y - df.obs$Yest\n",
    "v.sR2 = sum(df.obs$Res^2)/(n.obs-2)\n",
    "df.obs$Yhatup = df.obs$Yest + 2*sqrt(v.sR2)\n",
    "df.obs$Yhatdown = df.obs$Yest - 2*sqrt(v.sR2)\n",
    "\n",
    "lim.down = y.mn - 2*y.sd\n",
    "lim.up = y.mn + 2*y.sd\n",
    "vlim.l = min(vlim.l,lim.down)\n",
    "vlim.u = max(vlim.u,lim.up)\n",
    "\n",
    "plt.jit.y = df.obs %>% ggplot() + geom_jitter(aes(x=0,y=Y),height = 0) +\n",
    "  geom_hline(aes(yintercept=y.mn),color=\"blue\",size=0.75) +\n",
    "  geom_hline(aes(yintercept=lim.up),color=\"blue\",linetype=\"dashed\",size=0.75) +\n",
    "  geom_hline(aes(yintercept=lim.down),color=\"blue\",linetype=\"dashed\",size=0.75) +\n",
    "  ggtitle(\"Variability in the variable Y\") +\n",
    "  ylim(vlim.l,vlim.u) +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "\n",
    "plt.var.y = df.obs %>% ggplot() + geom_point(aes(x=X,y=Y)) +\n",
    "  geom_line(aes(x=X,y=Yest),color=\"blue\",size=0.75) +\n",
    "  geom_line(aes(x=X,y=Yhatup),color=\"blue\",linetype=\"dashed\",size=0.75) +\n",
    "  geom_line(aes(x=X,y=Yhatdown),color=\"blue\",linetype=\"dashed\",size=0.75) +\n",
    "  ggtitle(\"Variability associated to the regression model\") +\n",
    "  ylim(vlim.l,vlim.u) +\n",
    "  theme(plot.title = element_text(color=\"blue\", size=14, face=\"bold.italic\", hjust=0.5))\n",
    "\n",
    "suppressWarnings(grid.arrange(plt.jit.y,plt.var.y,nrow = 2,\n",
    "                              top=textGrob(sprintf(\"Comparing uncertainty sizes without and with the model\",\n",
    "                                                   s.pref$title),\n",
    "                                           gp=gpar(fontsize=15,col=\"blue\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc27141-9d17-4a96-a0d2-08778f7cb8f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "#### <span style=\"color:brown;\">The coefficient of determination, $R^2$</span>\n",
    "\n",
    "We can generate additional measures for the quality of the regression model by comparing the variabilities in the dependent variable and in the residuals. A way to define these measures, which has the advantage of having an easy interpretation, is to look at the sum of squares of distances to some center associated with this model.\n",
    "\n",
    "For example, the sample quasivariance of $Y$ is defined as a sum of squares divided by $n-1$. We will define this sum of squares as the <span style=\"color:brown;\">total sum of squares</span> ($\\text{SST}$) for our model:\n",
    "\n",
    "$$\n",
    "s_y^2 = \\frac{1}{n-1} \\sum_{i=1}^n (y_i - \\bar y)^2 = \\frac{\\text{SST}}{n-1} , \\qquad \\text{SST} = \\sum_{i=1}^n (y_i - \\bar y)^2 = (n-1) s_y^2\n",
    "$$\n",
    "\n",
    "This total sum of squares is a measure of the distance between each observation of the dependent variable and its mean, that is, a measure of the error we would be making if we use the mean of $Y$, $\\bar Y$, to approximate the values $Y_i$. This is our best alternative if we do not have any other information available.\n",
    "\n",
    "Let $\\hat y_i \\equiv \\hat \\beta_0 + \\hat \\beta_1 x_i$, the predicted value under the linear regression model. The sum of squares in $\\text{SST}$ can be written as\n",
    "\n",
    "$$\n",
    "\\text{SST} = \\sum_{i=1}^n (y_i - \\bar y)^2 = \\sum_{i=1}^n (y_i - \\hat y_i)^2 + \\sum_{i=1}^n (\\hat y_i - \\bar y)^2 = \\sum_{i=1}^n e_i^2 + \\sum_{i=1}^n (\\hat y_i - \\bar y)^2 = \\text{SSR} + \\text{SSM}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{SSR} & = & \\sum_{i=1}^n e_i^2 = (n-2) s_R^2 \\\\\n",
    "\\text{SSM} & = & \\sum_{i=1}^n (\\hat y_i - \\bar y)^2 = (n-1) s_y^2 - (n-2) s_R^2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "A justification for this result can be found in [Appendix 5](#App4_5).\n",
    "\n",
    "We have introduced the following sums of squares:\n",
    "- $\\text{SSM}$ denotes the <span style=\"color:brown;\">sum of squares of the model,</span> that is, the sum of the squares of the differences between the predicted values under the model and the mean of $Y$. These quantities indicate how much we gain by using the model to obtain better values for $Y$, based on the values of $X$.\n",
    "- $\\text{SSR}$ denotes the <span style=\"color:brown;\">sum of squares of the residuals,</span> that represents the sum of the squares of the errors left unexplained by the regression model, that is, the errors after we use the model to predict a value for $Y_i$, given the corresponding value of the independent variable $x_i$. Its size is a measure of how much variability we are unable to explain after taking into account the values of $X$ and the regression model.\n",
    "\n",
    "These sums of squares follow $\\chi^2$ distributions with different degrees of freedom, see [Appendix 5](#App4_5). This property will allow us to conduct inference on their values.\n",
    "\n",
    "These values also have a simple interpretation regarding our model. The value of the sum of squares of the residuals should be small with respect to the total sum of squares whenever we have a linear regression model that provides a very good approximation for the variable $Y$, given the values of $X$. We will use this property to introduce another measure for the quality of the linear regression model.\n",
    "\n",
    "We define the value of the <span style=\"color:brown;\">coefficient of determination, $R^2$,</span> of a linear regression model by comparing the sums of squares of the errors in the model (residuals) with the sum of squares of the differences between the observed values of the dependent variable and its mean,\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{\\mbox{SSM}}{\\mbox{SST}} = 1 - \\frac{\\mbox{SSR}}{\\mbox{SST}} = 1 - \\frac{\\sum_i e_i^2}{\\sum_i (y_i - \\bar y)^2} = 1 - \\frac{(n-2) s_R^2}{(n-1) s_y^2}\n",
    "$$\n",
    "\n",
    "This value provides a measure of the explanatory power of the model, as it can be interpreted as the proportion of the total variability in the dependent variable that is explained by the regression model.\n",
    "\n",
    "The coefficient of determination has the following properties:\n",
    "- It takes values in $[0,1]$. If its value is close to 1, the regression model provides a nearly perfect fit, while if it is close to zero, the model provides very little additional information about the dependent variable.\n",
    "- It represents the proportion of the total variability of the dependent variable that is explained by the regression model and the values of the independent variable. Thus, it has an immediate interpretation in terms of the explained variability.\n",
    "- It is closely related to the correlation coefficient, as $R^2 = \\text{cor}(x,y)^2$.\n",
    "\n",
    "A justification for this last result can be found in [Appendix 5](#App4_5).\n",
    "\n",
    "In the next lesson we introduce a modified measure, the <span style=\"color:brown;\">adjusted coefficient of determination,</span> and we provide a motivation for this definition. It is obtained as\n",
    "\n",
    "$$\n",
    "\\mbox{adj. } R^2 = 1 - \\frac{s_R^2}{s_y^2}\n",
    "$$\n",
    "\n",
    "The following cell obtains the values of these measures for our sample data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9cbc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Measuring the quality of the model\n",
    "## Computing the sums of squares\n",
    "\n",
    "SSR = (n.obs - 2)*v.sR2\n",
    "SST = (n.obs - 1)*y.sd^2\n",
    "R.2 = 1 - SSR/SST\n",
    "R.2.adj = 1 - v.sR2/y.sd^2\n",
    "\n",
    "out.0 <- round(as.data.frame(matrix(c(R.2,R.2.adj),1,2)),3)\n",
    "colnames(out.0) = c(\"Value\",\"Adj value\")\n",
    "rownames(out.0) = c(\"Coefficient of determination:\")\n",
    "\n",
    "table_prnt(out.0,\"Coefficient of determination\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19512af3-b890-496e-b133-9e89a8025565",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style=\"color:red\">Exercise</span>\n",
    "\n",
    "*The sales department of a clothing company is conducting a study on the company's online sales. Their goal is to determine if there is a meaningful relationship between the number of daily visits to its web page ($V$, measured in thousands) and the daily volume of Internet sales ($S$, measured in thousands of euros). The department has the following data on the values of these variables for the last 20 days:*\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\sum_{i=1}^{20} v_i & = & 599 , \\quad \\sum_{i=1}^{20} s_i = 2835 \\\\\n",
    "\\sum_{i=1}^{20} v_i^2 & = & 19195 , \\quad \\sum_{i=1}^{20} s_i^2 = 458657 , \\quad \\sum_{i=1}^{20} v_i s_i = 92000 \\\\\n",
    "\\sum_{i=1}^{20} e_i^2 & = & 16720.67\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "*where $e_i$ denotes the residuals of the regression model explaining the variable $S$ as a function of $V$.*\n",
    "\n",
    "- *Compute the value of the coefficient of determination and interpret it.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0641fe93-e617-4d05-b031-0e0daa58f8a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "##### <span style=\"color:red\">Exercise. Solution</span>\n",
    "\n",
    "We define our variables $V =$ \"Number of visits in a given day (in thousands)\" and $S =$ \"Daily Internet sales in hundreds of euros\". $V$ will be our independent variable.\n",
    "\n",
    "To obtain the estimated regression line we compute:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\bar v & = & \\displaystyle \\frac{1}{20} \\sum_{i=1}^{20} v_i = \\frac{599}{20} = 29.95 , \\quad \\bar s = \\frac{1}{20} \\sum_{i=1}^{20} s_i = \\frac{2835}{20} = 141.75 \\\\\n",
    "s_v^2 & = & \\displaystyle \\frac{1}{19} \\left( \\sum_{i=1}^{20} v_i^2 - 20 \\bar v^2 \\right) = \\frac{1}{19} ( 19195 - 20\\times 29.95^2 ) = 66.05 \\\\\n",
    "s_s^2 & = & \\displaystyle \\frac{1}{19} \\left( \\sum_{i=1}^{20} s_i^2 - 20 \\bar s^2 \\right) = \\frac{1}{19} ( 458657 - 20\\times 141.75^2 ) = 2989.25 \\\\\n",
    "\\text{cov} (v,s) & = & \\displaystyle \\frac{1}{19} \\left( \\sum_{i=1}^{20} v_i s_i - 20 \\bar v \\bar s \\right) = \\frac{1}{19} ( 92000 - 20\\times 29.95 \\times 141.75 ) = 373.25\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "From these values we obtain the least squares estimates,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\hat \\beta_1 & = & \\displaystyle \\frac{\\text{cov}(v,s)}{s_v^2} = \\frac{373.25}{66.05} = 5.651 \\\\\n",
    "\\hat \\beta_0 & = & \\bar s - \\hat \\beta_1 \\bar v = 141.75 - 5.651\\times 29.95 = -27.498\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "and the residual variance is\n",
    "\n",
    "$$\n",
    "s_R^2 = \\frac{\\sum_i e_i^2}{n-2} = \\frac{16720.67}{18} = 928.926\n",
    "$$\n",
    "\n",
    "The coefficient of determination can be computed from the correlation coefficient,\n",
    "\n",
    "$$\n",
    "\\text{cor} (v,s) = \\frac{\\text{cov}(v,s)}{s_v s_s} = \\frac{373.25}{\\sqrt{66.05\\times 2989.25}} = 0.840\n",
    "$$\n",
    "\n",
    "and we obtain\n",
    "\n",
    "$$\n",
    "R^2 = \\text{cor} (v,s)^2 = 0.840^2 = 0.7056\n",
    "$$\n",
    "\n",
    "Alternatively, we could have computed this value as\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSR}}{(n-1)s_s^2} = 1 - \\frac{16720.67}{19\\times 2989.25} = 0.7038\n",
    "$$\n",
    "\n",
    "The interpretation of this value is that the variable $V$, through the linear regression model, is able to explain 70% of the variability in the variable $S$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab2fbf-e61b-488f-9920-b8f1874426c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 style=\"color:brown;\">ANOVA table</h3>\n",
    "\n",
    "The sums of squares we have introduced above are very useful to analyze the quality of the linear regression model. In particular, they provide information regarding the explanatory power of the model, through the value of the coefficient of determination $R^2$. We will see that they also contain information with respect to the significance of the model.\n",
    "\n",
    "From a practical point of view, it is useful to group these values into an ANOVA table. The name ANOVA corresponds to the initials of <span style=\"color:brown;\">*ANalisys Of VAriance*,</span> which is what we have been doing in this last part of the lesson.\n",
    "\n",
    "The ANOVA table organizes the preceding values in the following form:\n",
    "\n",
    "$$\n",
    "\\small\n",
    "\\begin{array}{lcccc}\n",
    "\\text{Variability source} & \\text{Sums of squares} & \\text{Deg of freedom} & \\text{Means of squares} & \\text{F ratio} \\\\\n",
    "\\hline\n",
    "\\text{Model} & \\text{SSM} = \\sum_i (\\hat y_i - \\bar y)^2 & 1 & \\text{SSM}/1 & \\text{SSM}/s_R^2 \\\\\n",
    "\\text{Residuals} & \\text{SSR} = \\sum_i (y_i - \\hat y_i)^2 = \\sum_i e_i^2 & n-2 & \\text{SSR}/(n-2) = s_R^2 & \\\\\n",
    "\\hline\n",
    "\\text{Total} & \\text{SST} = \\sum_i (y_i - \\bar y)^2 = (n-1) s_y^2 & n-1 & & \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "To complete the table we:\n",
    "1. Start with the values of the sums of squares, obtained from the information of the model, and the degrees of freedom\n",
    "2. Compute the means of squares by dividing the sums of squares by the degrees of freedom\n",
    "3. Compute the <span style=\"color:brown;\">F Ratio</span> as the quotient between the sum of squares of the model and the residual variance\n",
    "\n",
    "The <span style=\"color:brown;\">F ratio</span> provides important information about the model: it allows us to test for the significance of this model. When the model is significant we should have a large value for the sum of squares of the model and a small value for the sum of squares of the residuals, implying a large value for the F Ratio.\n",
    "\n",
    "It also has a know distribution: it follows a Fisher F distribution with 1 and $n-2$ degrees of freedom; for a justification see [Appendix 6](#App4_6).\n",
    "\n",
    "In summary, we have for the F-Ratio\n",
    "\n",
    "$$\n",
    "\\text{F Ratio} = \\frac{\\text{SSM}}{S_R^2} \\sim F_{1,n-2} .\n",
    "$$\n",
    "\n",
    "and we can use its value and distribution to test for the significance of the simple linear regression model, $H_0 : \\beta_1 = 0$, by computing the p-value associated to the F ratio test, as\n",
    "\n",
    "$$\n",
    "\\text{p-value} = \\Pr \\left( F_{1,n-2} > \\text{F Ratio} \\right)\n",
    "$$\n",
    "\n",
    "A proof of this result can be found in [Appendix 6](#App4_6).\n",
    "\n",
    "The <span style=\"color:brown;\">F ratio</span> is also related to the value of the <span style=\"color:brown;\">coefficient of determination,</span> through\n",
    "\n",
    "$$\n",
    "\\text{F Ratio} = (n-2) \\frac{\\text{SSM}/\\text{SST}}{\\text{SSR}/\\text{SST}} = (n-2) \\frac{R^2}{1-R^2}\n",
    "$$\n",
    "\n",
    "In the following cell we compute the ANOVA table for the linear regression model and conduct the F ratio significance test for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f0c50-05d8-4c57-8dfe-996972d7fb8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# ANOVA table\n",
    "\n",
    "## Values for the table\n",
    "\n",
    "df.m = 1\n",
    "df.r = n.obs-2\n",
    "\n",
    "F.anova.s = (df.r/df.m)*(SST-SSR)/SSR\n",
    "xy.anova = matrix(c(SST-SSR,SSR,SST,df.m,df.r,n.obs-1,SST-SSR,SSR/df.r,NA,\n",
    "                    F.anova.s,NA,NA),3,4)\n",
    "xy.anova = as.data.frame(xy.anova)\n",
    "colnames(xy.anova) = c(\"SS\",\"DF\",\"Mean\",\"F ratio\")\n",
    "rownames(xy.anova) = c(\"Model\",\"Residuals\",\"Total\")\n",
    "\n",
    "## Print the ANOVA table\n",
    "\n",
    "xy.anova.f = xy.anova\n",
    "xy.anova.f[,1:4] = format(xy.anova[,1:4],digits = 3)\n",
    "xy.anova.f[2,4] = NA\n",
    "xy.anova.f[3,3:4] = NA\n",
    "\n",
    "options(knitr.kable.NA = ' ')\n",
    "options(align = 'r')\n",
    "table_prnt(xy.anova.f,\"ANOVA Table\")\n",
    "\n",
    "## Values for the significance test\n",
    "\n",
    "sig.lvl = 0.05\n",
    "val.b1.t = hat.beta.1/sqrt(v.sR2/((n.obs-1)*x.sd^2))\n",
    "crit.b1.t = qt(sig.lvl/2,df.r,lower.tail=FALSE)\n",
    "F.crit = qf(sig.lvl,df.m,df.r,lower.tail=FALSE)\n",
    "F.pval = pf(F.anova.s,df.m,df.r,lower.tail=FALSE)\n",
    "\n",
    "n.val.1 = 5\n",
    "val.t = c(sig.lvl,val.b1.t,crit.b1.t,F.anova.s,F.crit,F.pval)\n",
    "out.t = as.data.frame(matrix(format(val.t[1:n.val.1],digits=4),n.val.1,1))\n",
    "out.t = rbind(out.t,format(val.t[n.val.1+1],scientific=TRUE,digits=3))\n",
    "rownames(out.t) = c(\"Significance level\",\"Test beta 1\",\"Critical value beta 1\",\"F Ratio\",\n",
    "                    \"Critical value F ratio\",\"P value for the test\")\n",
    "colnames(out.t) = c(\"Values\")\n",
    "\n",
    "table_prnt(out.t,\"ANOVA significance test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54cbbbe-367b-4c01-9a86-83d606806dd6",
   "metadata": {},
   "source": [
    "\n",
    "#### <span style=\"color:red\">Exercise</span>\n",
    "\n",
    "*The sales department of a clothing company is conducting a study on the company's online sales. Their goal is to determine if there is a meaningful relationship between the number of daily visits to its web page ($V$, measured in thousands) and the daily volume of Internet sales ($S$, measured in thousands of euros). The department has the following data on the values of these variables for the last 20 days:*\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\sum_{i=1}^{20} v_i & = & 599 , \\quad \\sum_{i=1}^{20} s_i = 2835 \\\\\n",
    "\\sum_{i=1}^{20} v_i^2 & = & 19195 , \\quad \\sum_{i=1}^{20} s_i^2 = 458657 , \\quad \\sum_{i=1}^{20} v_i s_i = 92000 \\\\\n",
    "\\sum_{i=1}^{20} e_i^2 & = & 16720.67\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "*where $e_i$ denotes the residuals of the regression model explaining the variable $S$ as a function of $V$.*\n",
    "\n",
    "- *Compute the ANOVA table for $S$.*\n",
    "- *From the information in the ANOVA table, conduct a test for the significance of the linear regression model.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d860fc-a91f-4da5-be73-bde95e6acb6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "##### <span style=\"color:red\">Exercise. Solution</span>\n",
    "\n",
    "We define our variables $V =$ \"Number of visits in a given day (in thousands)\" and $S =$ \"Daily Internet sales in hundreds of euros\". $V$ will be our independent variable.\n",
    "\n",
    "We have already computed the estimates for the parameters of the model,\n",
    "\n",
    "$$\n",
    "\\hat \\beta_1 = 5.651 , \\qquad \\hat \\beta_0 = -27.498 , \\qquad s_R^2 = 928.926\n",
    "$$\n",
    "\n",
    "To obtain the ANOVA table we start with the sums of squares,\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{SSR} & = & \\sum_i e_i^2 = 16720.67 , \\quad \\text{SST} = \\sum_i (s_i - \\bar s)^2 = (n-1) s_s^2 = 19\\times 2989.25 = 56795.75 , \\\\\n",
    "\\text{SSM} & = & \\text{SST} - \\text{SSR} = 56795.75 - 16720.67 = 40075.08\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The remaining values will be given by\n",
    "\n",
    "$$\n",
    "s_R^2 = \\frac{16720.67}{18} = 928.926 , \\qquad \\text{F ratio} = \\frac{\\text{SSM}}{s_R^2} = \\frac{40075.08}{928.926} = 43.141\n",
    "$$\n",
    "\n",
    "And the ANOVA table will be\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcccc}\n",
    "\\text{Source} & \\text{Sums of squares} & \\text{Deg of freedom} & \\text{Means of squares} & \\text{F ratio} \\\\\n",
    "\\hline\n",
    "\\text{Model} & 40075.08 & 1 & 40075.08 & 43.141 \\\\\n",
    "\\text{Residuals} & 16720.67 & 18 & 928.926 & \\\\\n",
    "\\hline\n",
    "\\text{Total} & 56795.75 & 19 & & \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The significance test can be conducted from the value of the F ratio, as this value follows a Fisher F distribution with 1 and 18 degrees of freedom. The critical value in our case, for a significance level of 5%, is $F_{1,18;0.05} = 4.414$, and the value in the ANOVA table is clearly in the rejection region. We conclude that there is a significant linear relationship between the variables $S$ and $V$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5273e2e9-06fa-4f9f-b163-9e70bec8f4c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "---\n",
    "\n",
    "<a id='App4_1'></a>\n",
    "\n",
    "## <span style=\"color:orange;\">Appendix 1: Other parameter estimation methods</span>\n",
    "\n",
    "---\n",
    "\n",
    "The least squares method is not the only procedure that can be used to estimate the values of the parameters of the linear regression model. Other alternatives, with different properties for the estimators derived from them, are for example:\n",
    "\n",
    "- Least-squares estimation formulas minimizing other definitions of the residuals. If we define the residuals as the length of the shortest segment from observation $(x_i,y_i)$ to the regression line $y = \\hat \\beta_0 + \\hat \\beta_1 x$, the estimators are given as the solutions of\n",
    "\n",
    "$$\n",
    "\\min_{\\hat \\beta_0,\\hat \\beta_1} \\sum_{i=1}^n \\frac{(y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i)^2}{1 + \\hat \\beta_1^2}\n",
    "$$\n",
    "\n",
    "- Least absolute values regression. In order to reduce the influence of outlier observations, we may replace the square in the least squares problem with an absolute value, which gives less weight to any outlier observation. The estimators in this case are given by\n",
    "\n",
    "$$\n",
    "\\min_{\\hat \\beta_0,\\hat \\beta_1} \\sum_{i=1}^n | y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i |\n",
    "$$\n",
    "\n",
    "- Ridge regression. The least squares formulation can be regularized to ensure that the covariance matrix (in the multiple regression model) has reasonable numerical properties. This regularization can be done by adding penalization terms for the squared coefficients,\n",
    "\n",
    "$$\n",
    "\\min_{\\hat \\beta_0,\\hat \\beta_1} \\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i)^2 + \\rho \\left( \\hat \\beta_0^2 + \\hat \\beta_1^2 \\right)\n",
    "$$\n",
    "\n",
    " - Lasso regression. The regularization can also be carried out by adding penalization terms on the absolute values of the coefficients. This alternative has the advantage of allowing for the automatic selection of significant coefficients (coefficients different from zero) in the model.\n",
    "\n",
    "$$\n",
    "\\min_{\\hat \\beta_0,\\hat \\beta_1} \\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i)^2 + \\rho \\left( | \\hat \\beta_0 | + | \\hat \\beta_1 | \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a69c8-e4c9-4b51-a47c-c9e4f2b95681",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='App4_2'></a>\n",
    "\n",
    "## <span style=\"color:orange\">Appendix 2: Least squares estimates</span>\n",
    "\n",
    "---\n",
    "\n",
    "### <span style=\"color:orange\">Deriving the formulas of the least squares estimates</span>\n",
    "\n",
    "Given the least squares optimization problem,\n",
    "\n",
    "$$\n",
    "\\min_{\\hat \\beta_0,\\hat \\beta_1} f(\\hat \\beta_0,\\hat \\beta_1) = \\min_{\\hat \\beta_0,\\hat \\beta_1} \\sum_{i=1}^n e_i (\\hat \\beta_0,\\hat \\beta_1)^2 = \\min_{\\hat \\beta_0,\\hat \\beta_1} \\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "The first-order optimality conditions are\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{rcl}\n",
    "\\displaystyle \\frac{\\partial f}{\\partial \\hat \\beta_0} & = & \\displaystyle \\sum_{i=1}^n 2 (-1) (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i) = 0 \\\\\n",
    "\\displaystyle \\frac{\\partial f}{\\partial \\hat \\beta_1} & = & \\displaystyle \\sum_{i=1}^n 2 (-x_i) (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i) = 0\n",
    "\\end{array} \\right\\}\n",
    "\\quad \\Rightarrow \\quad\n",
    "\\left. \\begin{array}{rcl}\n",
    "\\displaystyle \\sum_{i=1}^n y_i & = & \\displaystyle n \\hat \\beta_0 + \\hat \\beta_1 \\sum_{i=1}^n x_i \\\\\n",
    "\\displaystyle \\sum_{i=1}^n x_i y_i & = & \\hat \\beta_0 \\sum_{i=1}^n x_i + \\hat \\beta_1 \\sum_{i=1}^n x_i^2\n",
    "\\end{array} \\right\\}\n",
    "$$\n",
    "\n",
    "If we reorder the terms in the preceding equations, these conditions are equivalent to\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "& & \\left. \\begin{array}{rcl}\n",
    "\\displaystyle \\frac{1}{n} \\sum_{i=1}^n y_i & = & \\displaystyle \\hat \\beta_0 + \\hat \\beta_1 \\frac{1}{n} \\sum_{i=1}^n x_i = 0 \\\\\n",
    "\\displaystyle \\sum_{i=1}^n x_i y_i - n \\bar x \\bar y & = & \\displaystyle\\hat \\beta_0 \\sum_{i=1}^n x_i + \\hat \\beta_1 \\sum_{i=1}^n x_i^2 - \\hat \\beta_1 n \\bar x^2 - n \\bar x \\bar y + \\hat \\beta_1 n \\bar x^2\n",
    "\\end{array} \\right\\} \\\\\n",
    "& \\Rightarrow \\quad &\n",
    "\\left. \\begin{array}{rcl}\n",
    "\\displaystyle \\bar y & = & \\displaystyle \\hat \\beta_0 + \\hat \\beta_1 \\bar x \\\\\n",
    "\\displaystyle \\frac{1}{n} \\left( \\sum_{i=1}^n x_i y_i - n \\bar x \\bar y \\right) & = & \\displaystyle \\hat \\beta_0 \\frac{1}{n} \\sum_{i=1}^n x_i + \\hat \\beta_1 \\frac{1}{n} \\left( \\sum_{i=1}^n x_i^2 - \\hat \\beta_1 \\bar x^2 \\right) - \\bar x \\left( \\bar y - \\hat \\beta_1 \\bar x \\right)\n",
    "\\end{array} \\right\\}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Replacing the first equation into the second one, and using the equalities\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\left( \\sum_{i=1}^n x_i y_i - n \\bar x \\bar y \\right) = \\frac{n-1}{n} \\text{cov} (x,y) , \\quad \\hat \\beta_1 \\frac{1}{n} \\left( \\sum_{i=1}^n x_i^2 - \\hat \\beta_1 \\bar x^2 \\right) = \\hat \\beta_1 \\frac{n-1}{n} s_x^2 ,\n",
    "$$\n",
    "\n",
    "we obtain from the first equation $\\hat \\beta_0 = \\bar y - \\hat \\beta_1 \\bar x$, and from the second one\n",
    "\n",
    "$$\n",
    "\\frac{n-1}{n} \\text{cov} (x,y) = \\displaystyle \\hat \\beta_0 \\bar x + \\hat \\beta_1 \\frac{n-1}{n} s_x^2 - \\bar x \\hat \\beta_0 = \\hat \\beta_1 \\frac{n-1}{n} s_x^2 \\quad \\Rightarrow \\quad \\hat \\beta_1 = \\frac{\\text{cov} (x,y)}{s_x^2}\n",
    "$$\n",
    "\n",
    "These are the two definitions that were introduced as the least-squares estimators for the two parameters defining the regression line.\n",
    "\n",
    "Finally, for the matrix of second derivatives,\n",
    "\n",
    "$$\n",
    "\\nabla^2 f(\\hat \\beta_0,\\hat \\beta_1) = \\left( \\begin{array}{cc} 2n & 2 \\sum_i x_i \\\\ 2 \\sum_i x_i & 2 \\sum_i x_i^2 \\end{array} \\right) ,\n",
    "$$\n",
    "\n",
    "and this matrix is positive definite whenever $n \\sum_i x_i^2 - (\\sum_i x_i )^2 > 0$, implying that the sufficient second-order optimality conditions are satisfied whenever $s_x^2 > 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903d0b0-c970-4272-98e8-b4b5ca574bf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='App4_3'></a>\n",
    "\n",
    "## <span style=\"color:orange\">Appendix 3: Parameter estimator distributions</span>\n",
    "\n",
    "---\n",
    "\n",
    "In this cell we justify why the estimators we have introduced for the different model parameters have the indicated distributions. We will base these justifications on our assumptions for the model, and on considering the values of the independent variable as given. That is, our results are conditional on the values of $X$.\n",
    "\n",
    "The values of the independent variable $X$ on which we will condition the estimators to obtain their distributions are denoted as $\\{ x_1, \\ldots , x_n \\}$, with mean $\\bar x$ and cuasivariance $s_x^2$.\n",
    "\n",
    "#### <span style=\"color:orange\">Distribution of the dependent observations, $Y_i$</span>\n",
    "\n",
    "From the definition of the linear regression model, $Y_i = \\beta_0 + \\beta_1 X_i + U_i$ and the assumptions on the model, we have that\n",
    "\n",
    "$$\n",
    "Y_i | \\left( \\underline{X} = \\{ x_1, \\ldots , x_n \\} \\right) = \\beta_0 + \\beta_1 x_i + U_i , \\quad U_i \\sim N(0,\\sigma^2) \\  \\Rightarrow \\  Y_i | \\underline{X} \\sim N(\\beta_0 + \\beta_1 x_i , \\sigma^2 )\n",
    "$$\n",
    "\n",
    "Also, from the independence of the errors $U_i$ it follows that the dependent variables $Y_i$ are also independent, when we condition on the values of $\\underline{X}$.\n",
    "\n",
    "In what follows, and to simplify the notation, we will skip the conditional notation, but we will continue assuming this condition applies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2bc405-6856-4fee-805a-9905c5489a58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:orange\">Distribution of the statistic for the slope of the regression line, $\\hat \\beta_1$</span>\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\hat \\beta_1 & = & \\displaystyle \\frac{\\text{cov}(x,Y)}{s_x^2} = \\frac{1}{s_x^2} \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar x) (Y_i - \\bar Y) = \\sum_{i=1}^n w_i (Y_i - \\bar Y) = \\sum_{i=1}^n w_i Y_i - \\bar Y \\sum_{i=1}^n w_i \\\\\n",
    "& = & \\sum_{i=1}^n w_i Y_i\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where we have introduced the notation\n",
    "\n",
    "$$\n",
    "\\begin{array}{rclcl}\n",
    "\\displaystyle v_i & \\equiv & x_i - \\bar x , & \\text{ which satisfy } & \\sum_{i=1}^n v_i = \\sum_{i=1}^n w_i = 0 \\\\\n",
    "\\displaystyle w_i & \\equiv & \\displaystyle \\frac{v_i}{(n-1) s_x^2} & & \\sum_{i=1}^n v_i^2 = \\sum_{i=1}^n v_i (x_i - \\bar x) = (n-1) s_x^2 = \\sum_{i=1}^n v_i x_i \\\\\n",
    "& & & \\text{ and } & \\sum_{i=1}^n w_i (x_i - \\bar x) = \\sum_{i=1}^n w_i x_i = 1\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This result implies that $\\hat \\beta_1$ follows a normal distribution, as the $Y_i$ are normally distributed.\n",
    "\n",
    "The mean and variance of $\\hat \\beta_1$ are given by\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "E[\\hat \\beta_1] & = & \\sum_{i=1}^n w_i E[Y_i] = \\sum_{i=1}^n w_i (\\beta_0 + \\beta_1 x_i ) = \\beta_0 \\sum_{i=1}^n w_i + \\beta_1 \\sum_{i=1}^n w_i x_i = \\beta_1 \\\\\n",
    "\\text{Var} (\\hat \\beta_1) & = & \\displaystyle \\text{Var} \\left( \\sum_{i=1}^n w_i Y_i \\right) = \\sum_{i=1}^n w_i^2 \\text{Var} (Y_i) = \\sum_{i=1}^n w_i^2 \\sigma^2 = \\frac{\\sigma^2}{(n-1)s_x^2}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where we have made use of the independence of the $Y_i$ and $\\text{Var} (Y_i) = \\sigma^2$. One consequence of these results is that $\\hat \\beta_1$ is an unbiased estimator for $\\beta_1$ under our assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56324e-2c61-43c0-9e38-b855c9e7f1b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:orange\">Distribution of the statistic for the intercept of the regression line, $\\hat \\beta_0$</span>\n",
    "\n",
    "It holds that\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\hat \\beta_0 & = & \\bar Y - \\hat \\beta_1 \\bar x = \\bar Y - \\sum_{i=1}^n w_i (Y_i - \\bar Y) \\bar x = \\frac{1}{n} \\sum_{i=1}^n Y_i - \\sum_{i=1}^n \\bar x w_i Y_i + \\bar Y \\bar x \\sum_{i=1}^n w_i \\\\\n",
    "& = & \\frac{1}{n} \\sum_{i=1}^n \\left( 1 - n \\bar x w_i \\right) Y_i\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "implying that $\\hat \\beta_0$ also follows a normal distribution.\n",
    "\n",
    "Its mean and variance are given by\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "E[\\hat \\beta_0] & = & \\displaystyle \\frac{1}{n} \\sum_{i=1}^n \\left( 1 - n \\bar x w_i \\right) E[Y_i] = \\frac{1}{n} \\sum_{i=1}^n \\left( 1 - n \\bar x w_i \\right) ( \\beta_0 + \\beta_1 x_i) \\\\\n",
    "& = & \\displaystyle \\beta_0 + \\beta_1 \\frac{1}{n} \\sum_{i=1}^n \\left( 1 - n \\bar x w_i \\right) x_i = \\beta_0 + \\beta_1 \\left( \\bar x - \\bar x \\sum_{i=1}^n w_i x_i \\right) = \\beta_0 \\\\\n",
    "\\text{Var} (\\hat \\beta_0) & = & \\displaystyle \\text{Var} \\left( \\frac{1}{n} \\sum_{i=1}^n \\left( 1 - n \\bar x w_i \\right) Y_i \\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\left( 1 - n \\bar x w_i \\right)^2 \\text{Var} (Y_i) \\\\\n",
    "& = & \\displaystyle \\frac{\\sigma^2}{n^2} \\sum_{i=1}^n \\left( 1 - n \\bar x w_i \\right)^2 = \\frac{\\sigma^2}{n^2} n + \\frac{\\sigma^2}{n^2} n^2 \\bar x^2 \\sum_{i=1}^n w_i^2 = \\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar x^2}{(n-1) s_x^2} \\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "and we also have that $\\hat \\beta_0$ is an unbiased estimator for $\\beta_0$ under our assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75763bc2-13ee-407d-94ac-5ef6c52ae1ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:orange\">Distribution of the estimator for the variance of the errors, $s_R^2$</span>\n",
    "\n",
    "<span style=\"color:brown\">The following derivation makes use of results from matrix algebra and multivariate statistics.</span>\n",
    "\n",
    "From the definition of the residual variance,\n",
    "\n",
    "$$\n",
    "S_R^2 = \\frac{1}{n-2} \\sum_{i=1}^n E_i^2 = \\frac{1}{n-2} \\sum_{i=1}^n \\left(Y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i \\right)^2\n",
    "$$\n",
    "\n",
    "And from the definition of the residuals\n",
    "\n",
    "$$\n",
    "E_i = Y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i ,\n",
    "$$\n",
    "\n",
    "it holds that these residuals $E_i$ are normally distributed, as they are a linear combination of normal random variables, $Y_i$, $\\hat \\beta_0$ and $\\hat \\beta_1$.\n",
    "\n",
    "Its expected values are\n",
    "\n",
    "$$\n",
    "E[E_i] = E[Y_i] - E[\\hat \\beta_0] - E[\\hat \\beta_1] x_i = \\beta_0 + \\beta_1 x_i - \\beta_0 - \\beta_1 x_i = 0\n",
    "$$\n",
    "\n",
    "To obtain the variance of the residuals it is easier to use matrix notation. In this notation we have\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "E & = & \\displaystyle Y - \\hat \\beta_0 e - \\hat \\beta_1 x = Y - e \\left( \\frac{1}{n} e - \\bar x w \\right)^T Y - x w^T Y \\\\\n",
    "& = & \\displaystyle Y - \\frac{1}{e^T e} e e^T Y + \\bar x e \\frac{1}{v^Tv} v^T Y - (v + \\bar x e) \\frac{1}{v^T v} v^T Y \\\\\n",
    "& = & \\displaystyle \\left( I - \\frac{1}{e^T e} e e^T - \\frac{1}{v^T v} v v^T \\right) Y = M Y\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where we have used the vector $e$, having all components equal to one, and the matrix of coefficients for $Y$, $M$.\n",
    "\n",
    "The covariance matrix for $E$ is given by\n",
    "\n",
    "$$\n",
    "\\text{Var} (E) = M \\text{Var} (Y) M^T = \\sigma^2 M = \\sigma^2 \\left( I - \\frac{1}{n} e e^T - \\frac{1}{(n-1)s_x^2} v v^T \\right)\n",
    "$$\n",
    "\n",
    "This result implies that the variances of the different residuals (the values in the diagonal of this matrix) are different,\n",
    "\n",
    "$$\n",
    "\\text{Var} (E_i) = \\sigma^2 \\left( 1 - \\frac{1}{n} - \\frac{1}{(n-1)s_x^2} (x_i - \\bar x)^2 \\right)\n",
    "$$\n",
    "\n",
    "and the residuals are not independent, as the values outside the diagonal of $M$ are not zero.\n",
    "\n",
    "Define an orthogonal matrix $Q \\in \\mathbb{R}^{n\\times (n-2)}$ with columns corresponding to an orthonormal basis for the subspace orthogonal to $\\text{span} (e,v)$ (orthogonal to the subspace generated by the vectors $e$ and $v$, and such that $Q^T e = Q^T v = 0$), and another matrix $\\tilde Q \\in \\mathbb{R}^{n\\times (n-2)}$ with columns corresponding to another orthonormal basis for $\\text{span} (e,v)$. Also, let $\\hat Q = \\left( \\begin{array}{cc} Q & \\tilde Q \\end{array} \\right)$.\n",
    "\n",
    "Define a new set of $n-2$ variables $R_i$ as the components of $R \\equiv Q^T E \\in \\mathbb{R}^{n-2}$; these variables are linear combinations of normal random variables, and as a consequence they follow a normal distribution. It holds that $\\text{Var} (R) = \\text{Var} (Q^T E) = Q^T \\text{Var} (E) Q = \\sigma^2 I$, that is, $R_i \\sim N(0,\\sigma^2)$, and these variables are independent of each other, as their covariance matrix is diagonal.\n",
    "\n",
    "It holds that\n",
    "\n",
    "$$\n",
    "e^T E = \\sum_i E_i = \\sum_i Y_i - n \\hat \\beta_0 - \\hat \\beta_1 \\sum_i x_i = n(\\bar Y - \\hat \\beta_0 - \\hat \\beta_1 \\bar x) = 0\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "v^T E & = & \\sum_i x_i E_i - \\bar x \\sum E_i = \\sum_i x_i Y_i - \\hat \\beta_0 \\sum_i x_i - \\hat \\beta_1 \\sum_i x_i^2 \\\\\n",
    "& = & \\sum_i x_i Y_i - \\bar Y \\sum_i x_i + \\hat \\beta_1 \\bar x \\sum_i x_i - \\hat \\beta_1 \\sum_i x_i^2 = \\sum_i x_i ( Y_i - \\bar Y ) - \\hat \\beta_1 \\sum_i x_i (x_i - \\bar x) \\\\\n",
    "& = & \\sum_i (x_i - \\bar x) ( Y_i - \\bar Y ) - \\hat \\beta_1 \\sum_i (x_i - \\bar x)^2 = 0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "implying that $d^T E = 0$ for any $d \\in \\text{span} (e,v)$.\n",
    "\n",
    "As $\\hat Q \\hat Q^T = I$, we have that\n",
    "\n",
    "$$\n",
    "E^T E = E^T \\hat Q \\hat Q^T E = E^T Q Q^T E + E^T \\tilde Q \\tilde Q^T E = R^T R + E^T \\tilde Q \\tilde Q^T E = R^T R ,\n",
    "$$\n",
    "\n",
    "and we have the desired result,\n",
    "\n",
    "$$\n",
    "T_{\\sigma^2} = \\frac{(n-2) S_R^2}{\\sigma^2} = \\frac{(n-2) E^T E/(n-2)}{\\sigma^2} = \\sum_{i=1}^{n-2} \\frac{R_i^2}{\\sigma^2} \\sim \\chi^2_{n-2}\n",
    "$$\n",
    "\n",
    "A consequence of this result is that\n",
    "\n",
    "$$\n",
    "E[T_{\\sigma^2}] = E[\\chi^2_{n-2}] = n - 2 = E\\left[ \\frac{(n-2) S_R^2}{\\sigma^2} \\right] = \\frac{n-2}{\\sigma^2} E [ S_R^2 ] \\ \\Rightarrow \\ E [ S_R^2 ] = \\sigma^2 \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d745d-064d-46c0-9401-707e183d254a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:orange\">Distribution of the estimator for $\\hat \\beta_1$</span>\n",
    "\n",
    "We have seen that\n",
    "\n",
    "$$\n",
    "Z \\equiv \\frac{\\hat \\beta_1 - \\beta_1}{\\displaystyle \\sqrt{\\frac{\\sigma^2}{(n-1)s_x^2}}} \\sim N(0,1) , \\qquad V \\equiv \\frac{(n-2)S_R^2}{\\sigma^2} \\sim \\chi^2_{n-2}\n",
    "$$\n",
    "\n",
    "implying from the definition of a Student t distribution (see Lesson 1) that\n",
    "\n",
    "$$\n",
    "\\frac{Z}{\\sqrt{V/(n-2)}} = \\frac{\\displaystyle(\\hat \\beta_1 - \\beta_1)/\\sqrt{\\frac{\\sigma^2}{(n-1)s_x^2}}}{\\displaystyle \\sqrt{\\frac{(n-2)S_R^2/\\sigma^2}{n-2}}} = \\frac{\\hat \\beta_1 - \\beta_1}{\\displaystyle \\sqrt{\\frac{S_R^2}{(n-1)s_x^2}}} = T_{\\beta_1} \\sim t_{n-2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6982c7d2-c7c6-4a15-8631-c0b88bd52954",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:orange\">Distribution of the estimator for $\\hat \\beta_0$</span>\n",
    "\n",
    "Analogously to the preceding case, we have that\n",
    "\n",
    "$$\n",
    "Z' \\equiv \\frac{\\hat \\beta_0 - \\beta_0}{\\displaystyle \\sqrt{\\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar x^2}{(n-1) s_x^2} \\right)}} \\sim N(0,1) , \\qquad V \\equiv \\frac{(n-2)S_R^2}{\\sigma^2} \\sim \\chi^2_{n-2}\n",
    "$$\n",
    "\n",
    "and we obtain\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\displaystyle \\frac{Z'}{\\sqrt{V/(n-2)}} & = & \\frac{\\displaystyle(\\hat \\beta_0 - \\beta_0)/\\sqrt{\\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar x^2}{(n-1) s_x^2} \\right)}}{\\displaystyle \\sqrt{\\frac{(n-2)S_R^2/\\sigma^2}{n-2}}} \\\\\n",
    "& = & \\displaystyle \\frac{\\hat \\beta_0 - \\beta_0}{\\sqrt{S_R^2 \\left( \\frac{1}{n} + \\frac{\\bar x^2}{(n-1) s_x^2} \\right)}} = T_{\\beta_0} \\sim t_{n-2}\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7a0ab-61e7-460b-abac-a24f51a0a15b",
   "metadata": {},
   "source": [
    "<a id='App4_4'></a>\n",
    "\n",
    "## <span style=\"color:orange;\">Appendix 4: Forecasting distributions</span>\n",
    "\n",
    "---\n",
    "\n",
    "#### <span style=\"color:orange;\">A formal definition for mean responses and forecasts</span>\n",
    "\n",
    "- <span style=\"color:brown;\">Forecast</span> estimates\n",
    "\n",
    "  Let $U_0 = U | (X = x_0)$. The random variable of interest whose expected value we would like to estimate in this case will be\n",
    "\n",
    "$$\n",
    "Y_0 = Y | (X = x_0) = ( \\beta_0 + \\beta_1 X + U ) | (X = x_0) = \\beta_0 + \\beta_1 x_0 + U_0\n",
    "$$\n",
    "  \n",
    "- <span style=\"color:brown;\">Mean response</span> estimates\n",
    "\n",
    "  We will denote the random variable of interest as $\\hat Y$. It will be given by\n",
    "\n",
    "$$\n",
    "\\hat Y = E[Y | X = x_0 ] = E [ \\beta_0 + \\beta_1 X + U | X = x_0 ] = \\beta_0 + \\beta_1 x_0\n",
    "$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb803a0e-eea1-4d36-8132-fabcab6fb786",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange;\">Derivation of the distributions</span>\n",
    "\n",
    "We now provide justification for the results regarding the distributions of the estimators for the mean response and the forecast.\n",
    "\n",
    "In both cases, as $\\hat Y_0$ is a linear combination of normal random variables, it will follow a normal distribution. Its mean, also in both cases, will be given by $y_0 = \\beta_0 + \\beta_1 x_0$. But the variance of the estimator will be different for the case of a mean response estimator or a forecast estimator.\n",
    "\n",
    "In what follows we omit the indication that we are computing a conditional variance for $X = x_0$, as well as the fact that we are conditioning on the known values of $X$, to simplify the notation.\n",
    "\n",
    "##### <span style=\"color:orange;\">Variance and distribution of the mean response estimator</span>\n",
    "\n",
    "To obtain the variance in this case, we use\n",
    "\n",
    "$$\n",
    "\\hat Y = \\hat \\beta_0  + \\hat \\beta_1 x_0 = \\bar Y - \\hat \\beta_1 \\bar x + \\hat \\beta_1 x_0 = \\bar Y + (x_0 - \\bar x) \\hat \\beta_1 = \\sum_{i=1}^n \\left( \\frac{1}{n} + (x_0 - \\bar x) w_i \\right) Y_i\n",
    "$$\n",
    "\n",
    "where $w_i$ are the values introduced in our previous justifications for the distributions of the linear regression parameter estimators.\n",
    "\n",
    "We obtain\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{Var} (\\hat Y) & = & \\displaystyle \\sum_{i=1}^n \\left( \\frac{1}{n} + (x_0 - \\bar x) w_i \\right)^2 \\text{Var} (Y_i) = \\sigma^2 \\sum_{i=1}^n \\left( \\frac{1}{n^2} + (x_0 - \\bar x)^2 w_i^2 \\right) \\\\\n",
    "& = & \\displaystyle \\sigma^2 \\left( \\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{(n-1)s_x^2} \\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "and as a consequence,\n",
    "\n",
    "$$\n",
    "\\frac{\\hat Y - y_0}{\\sigma \\sqrt{\\displaystyle \\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{(n-1)s_x^2}}} \\sim N(0,1) \\quad \\text{ and } \\quad T_{mr} = \\frac{\\hat Y - y_0}{s_R \\sqrt{\\displaystyle \\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{(n-1)s_x^2}}} \\sim t_{n-2}\n",
    "$$\n",
    "\n",
    "##### <span style=\"color:orange;\">Variance and distribution of the forecast estimator</span>\n",
    "\n",
    "We now have\n",
    "\n",
    "$$\n",
    "Y_0 = \\hat \\beta_0  + \\hat \\beta_1 x_0 + U_0 = \\sum_{i=1}^n \\left( \\frac{1}{n} + (x_0 - \\bar x) w_i \\right) Y_i + U_0\n",
    "$$\n",
    "\n",
    "As $x_0$ is different from the values in the sample $x_i$, from our assumptions $U_0 = U | X = x_0$ is independent of the errors $U_i = U | X = x_0$, and as a consequence it is also independent of the dependent variables $Y_i$. The variance will be given by\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{Var} (Y_0) & = & \\displaystyle \\sum_{i=1}^n \\left( \\frac{1}{n} + (x_0 - \\bar x) w_i \\right)^2 \\text{Var} (Y_i) + \\text{Var} (U_0) \\\\\n",
    "& = & \\displaystyle \\sigma^2 \\sum_{i=1}^n \\left( \\frac{1}{n^2} + (x_0 - \\bar x)^2 w_i^2 \\right) + \\sigma^2 = \\sigma^2 \\left( 1 + \\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{(n-1)s_x^2} \\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We then have\n",
    "\n",
    "$$\n",
    "\\frac{Y_0 - y_0}{\\sigma \\sqrt{\\displaystyle 1 + \\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{(n-1)s_x^2}}} \\sim N(0,1) \\quad \\text{ and } \\quad T_{mr} = \\frac{Y_0 - y_0}{s_R \\sqrt{\\displaystyle 1 + \\frac{1}{n} + \\frac{(x_0 - \\bar x)^2}{(n-1)s_x^2}}} \\sim t_{n-2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370844d7-7f1a-4e5d-8338-e4e3a71e22d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='App4_5'></a>\n",
    "\n",
    "## <span style=\"color:orange;\">Appendix 5: Sums of squares properties</span>\n",
    "\n",
    "In this cell we provide justifications for several results on the behavior of different sums of squares related to the linear regression model.\n",
    "\n",
    "#### <span style=\"color:orange;\">General relationship between sums of squares</span>\n",
    "\n",
    "The sums of squares of a linear regression model satisfy $\\text{SST} = \\text{SSM} + \\text{SSR}$. This is not a trivial result, as we are comparing sums of squares and in general $a^2 + b^2 \\not= (a+b)^2$. But in our case this result holds, as\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{SST} & = & \\sum_{i=1}^n (y_i - \\bar y)^2 = \\sum_{i=1}^n (y_i - \\hat y_i + \\hat y_i - \\bar y)^2 \\\\\n",
    "& = & \\sum_{i=1}^n (y_i - \\hat y_i)^2 + 2 \\sum_{i=1}^n (y_i - \\hat y_i)(\\hat y_i - \\bar y) + \\sum_{i=1}^n (\\hat y_i - \\bar y)^2 \\\\\n",
    "& = & \\text{SSR} + \\text{SSM} + 2 \\sum_{i=1}^n (y_i - \\hat y_i)(\\hat y_i - \\bar y)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\sum_{i=1}^n (y_i - \\hat y_i)(\\hat y_i - \\bar y) & = & \\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1 x_i)(\\hat \\beta_0 + \\hat \\beta_1 x_i y_i) \\\\\n",
    "& = & 2n \\hat \\beta_0 (\\bar y - \\hat \\beta_1 \\bar x) - n \\bar y (\\bar y - \\hat \\beta_1 \\bar x) - n\\hat \\beta_0^2 + \\hat \\beta_1 \\left( \\sum_i x_i y_i - \\hat \\beta_1 \\sum_i x_i^2 \\right) \\\\\n",
    "& = & n \\hat \\beta_0 (\\hat \\beta_0 - \\bar y) + \\hat \\beta_1 n \\bar x ( \\bar y - \\hat \\beta_1 \\bar x ) = n \\hat \\beta_0 \\hat \\beta_1 \\bar x - \\hat \\beta_0 \\hat \\beta_1 \\bar x = 0\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b71dd-acd6-48d4-8e86-6573bb50d22c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:orange;\">Distributions of the sums of squares</span>\n",
    "\n",
    "Regarding the distributions of the sums of squares, we have already shown that\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\displaystyle \\frac{(n-1)S_Y^2}{\\sigma^2} & \\sim & \\displaystyle \\chi^2_{n-1} \\quad \\Rightarrow \\quad \\frac{\\text{SST}}{\\sigma^2} \\sim \\chi^2_{n-1} \\\\\n",
    "\\displaystyle \\frac{(n-2)S_R^2}{\\sigma^2} & \\sim & \\displaystyle \\chi^2_{n-2} \\quad \\Rightarrow \\quad \\frac{\\text{SSR}}{\\sigma^2} \\sim \\chi^2_{n-2}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For the distribution of $\\text{SSM}$, it holds that\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\text{SSM} & = & \\sum_{i=1}^n \\left( \\hat Y_i - \\bar Y \\right)^2 = \\sum_{i=1}^n \\left( \\hat \\beta_0 + \\hat \\beta_1 x_i - \\bar Y \\right)^2 = \\sum_{i=1}^n \\left( \\bar Y - \\hat \\beta_1 \\bar x + \\hat \\beta_1 x_i - \\bar Y \\right)^2 \\\\\n",
    "& = & \\hat \\beta_1^2 \\sum_{i=1}^n (x_i - \\bar x)^2 = (n-1) s_x^2 \\hat \\beta_1^2 \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We have seen that\n",
    "\n",
    "$$\n",
    "\\hat \\beta_1 \\sim N \\left( \\beta_1 , \\frac{\\sigma^2}{(n-1)s_x^2} \\right)\n",
    "$$\n",
    "\n",
    "and as a consequence, if we condition on $\\beta_1 = 0$, that is, on not having a linear relationship between the variables,\n",
    "\n",
    "$$\n",
    "\\frac{\\text{SSM} | \\beta_1 = 0}{\\sigma^2} = \\frac{\\hat \\beta_1^2}{\\displaystyle \\frac{\\sigma^2}{(n-1)s_x^2}} \\sim \\chi^2_1\n",
    "$$\n",
    "\n",
    "that is, $\\text{SSM} | \\beta_1 = 0$ follows a chi squared distribution with one degree of freedom, as it can be written as the square of one standard normal random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f5d53-9825-439f-8568-d8f3a8a273ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <span style=\"color:orange;\">The coefficient of determination and the correlation coefficient</span>\n",
    "\n",
    "The relationship between the coefficient of determination and the correlation coefficient follows from\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{\\text{SSM}}{\\text{SST}} = \\hat \\beta_1^2 \\frac{(n-1) s_x^2}{(n-1) s_y^2} = \\frac{\\text{cov}(x,y)^2}{s_x^4} \\frac{s_x^2}{s_y^2} = \\frac{\\text{cov}(x,y)^2}{s_x^2 s_y^2} = \\text{cor}(x,y)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb78ec8-ce01-4e93-9237-bad302c978ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='App4_6'></a>\n",
    "\n",
    "## <span style=\"color:orange;\">Appendix 6: The F ratio</span>\n",
    "\n",
    "---\n",
    "\n",
    "Under $\\beta_1 = 0$ it holds that\n",
    "\n",
    "$$\n",
    "\\text{F Ratio} | (\\beta_1 = 0) = \\frac{\\text{SSM} | (\\beta_1 = 0)}{S_R^2} \\sim F_{1,n-2}\n",
    "$$\n",
    "\n",
    "This distribution is a consequence of the F Ratio being defined as the ratio between two chi squared random variables with degrees of freedom 1 and $n-2$, both with the same value of $\\sigma^2$.\n",
    "\n",
    "As we mentioned before, we can test for $H_0 : \\beta_1 = 0$, by finding the p-value associated to the distribution of the F ratio, as\n",
    "\n",
    "$$\n",
    "\\text{p-value} = \\Pr \\left( F_{1,n-2} > \\text{F Ratio} \\right)\n",
    "$$\n",
    "\n",
    "This test is closely related to the significance test introduced before for $\\hat \\beta_1$, as we have that\n",
    "\n",
    "$$\n",
    "\\text{F Ratio} = \\frac{\\text{SSM}}{\\text{SSR}/(n-2)} = \\frac{(n-1)s_x^2 \\hat \\beta_1^2}{s_R^2} = \\frac{\\hat \\beta_1^2}{\\displaystyle \\frac{s_R^2}{(n-1)s_x^2}} = T_{\\beta_1}^2 | (\\beta_1 = 0)\n",
    "$$\n",
    "\n",
    "that is, the value of the F ratio is the square of the value of the test statistic based on the distribution of $\\hat \\beta_1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb08b00-1a10-4451-8fc6-65b5577e9f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.0"
  },
  "rise": {
   "font-size": "0.5em",
   "theme": "sky"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
